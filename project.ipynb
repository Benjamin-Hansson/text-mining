{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient, ASCENDING\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client[\"text_mining\"]\n",
    "mongo_comments = db[\"comments\"]\n",
    "mongo_posts = db[\"posts\"]\n",
    "post_ids = mongo_comments.distinct('post_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/benjamin/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "new_words = {\n",
    "    \"bull\": 1,\n",
    "    \"bullish\": 1,\n",
    "    \"moon\": 1,\n",
    "    \"calls\": 1,\n",
    "    \"bear\": -1,\n",
    "    \"bearish\": -1,\n",
    "    \"green\": 1,\n",
    "    \"red\": -1,\n",
    "    \"tank\": -1,\n",
    "    \"puts\": -1\n",
    "}\n",
    "sid.lexicon.update(new_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_comments(comments):\n",
    "    score = []\n",
    "    for comment in comments:\n",
    "        score.append(sid.polarity_scores(comment[\"body\"]))\n",
    "    return score\n",
    "\n",
    "def analyze_text(text):\n",
    "    return sid.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_score_individual_comments(scores):\n",
    "    total_ammount_negative, total_ammount_positive, \n",
    "    total_polarity_positive, total_polarity_negative = 0\n",
    "\n",
    "    for score in scores:\n",
    "        ammount_negative, ammount_positive,\n",
    "        ammount_neutral, polarity = 0\n",
    "        \n",
    "        for entry in score:\n",
    "            if entry[\"compound\"] > 0:\n",
    "                ammount_positive += 1\n",
    "            elif entry[\"compond\"] == 0:\n",
    "                ammount_neutral += 1\n",
    "            else:\n",
    "                ammount_negative += 1\n",
    "            polarity += entry[\"compound\"]\n",
    "                \n",
    "        if negative < positive:\n",
    "            total_ammount_positive += 1\n",
    "        elif positive < negative:\n",
    "            total_ammount_negative += 1\n",
    "        if polarity > 0:\n",
    "            total_polarity_positive += 1\n",
    "        elif polarity < 0:\n",
    "            total_polarity_negative += 1  \n",
    "    return total_ammount_positive, total_ammount_negative, total_polarity_positive, total_polarity_negative\n",
    "        \n",
    "def analyze_score_combined_comments(scores):\n",
    "    total_negative = 0\n",
    "    total_positive = 0\n",
    "    for score in scores:\n",
    "        if score[\"compound\"] > 0:\n",
    "            total_positive += 1\n",
    "        elif score[\"compound\"] < 0:\n",
    "            total_negative += 1\n",
    "    return total_negative, total_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add sentiment score to all comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for post_id in post_ids:\n",
    "    \n",
    "    post = mongo_posts.find({\"id\": post_id})[0]\n",
    "    comments = mongo_comments.find({\"post_id\": post_id})\n",
    "    for comment in comments:\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(f\"Updating comment number {count}\")\n",
    "        polarity_score = sid.polarity_scores(comment[\"body\"])\n",
    "        mongo_comments.update_one({\"id\": comment[\"id\"],\"post_id\": post_id},{\"$set\": {\"sentiment_score\": polarity_score}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALPHA VANTAGE download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import pandas\n",
    "API_KEY = \"M8RF5RFRF8PG32I1\"\n",
    "ts = TimeSeries(key=\"M8RF5RFRF8PG32I1\", output_format='pandas')\n",
    "res = ts.get_daily_adjusted(symbol=\"SPY\", outputsize=\"full\")[0]\n",
    "res = res.drop([\"1. open\", \"2. high\", \"3. low\", \"4. close\", \"6. volume\", \"7. dividend amount\", \"8. split coefficient\"], axis=1)\n",
    "res = res.rename(columns={'5. adjusted close': \"close\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add close price to comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pytz import timezone\n",
    "\n",
    "one_day_offset = timedelta(days=1)\n",
    "\n",
    "for post_id in post_ids:\n",
    "    post = mongo_posts.find({\"id\": post_id})[0]\n",
    "    comments = mongo_comments.find({\"post_id\": post_id})\n",
    "    for comment in comments:\n",
    "        start_day = datetime.fromtimestamp(comment[\"created_utc\"]).date()\n",
    "        \n",
    "        date = datetime.fromtimestamp(comment[\"created_utc\"]).date() + one_day_offset\n",
    "        while res.loc[res.index == str(date)].empty:\n",
    "            date = date + one_day_offset \n",
    "        next_trading_day = date\n",
    "        \n",
    "        date = start_day\n",
    "        while res.loc[res.index == str(date)].empty:\n",
    "            date = date - one_day_offset\n",
    "        previous_trading_day = date\n",
    "        \n",
    "        next_close = float(res.loc[res.index == str(next_trading_day)][\"close\"])\n",
    "        prev_close = float(res.loc[res.index == str(previous_trading_day)][\"close\"])\n",
    "        \n",
    "        positive_day = None\n",
    "        \n",
    "        if next_close >= prev_close:\n",
    "            positive_day = True\n",
    "        else:\n",
    "            positive_day = False\n",
    "        development = 1 - (next_close/prev_close)\n",
    "        mongo_comments.update_one({\"id\": comment[\"id\"],\"post_id\": post_id},{\"$set\": {\"spy_closing_price\": next_close}})\n",
    "    \n",
    "        mongo_comments.update_one({\"id\": comment[\"id\"],\"post_id\": post_id},{\"$set\": {\"next_trading_day_positive\": positive_day}})\n",
    "        \n",
    "        mongo_comments.update_one({\"id\": comment[\"id\"],\"post_id\": post_id},{\"$set\": {\"next_trading_day_development\": development}})\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive_day_positive_sentiment=287403\n",
      "positive_day_negative_sentiment=246476\n",
      "positive_day_neutral_sentiment=303434\n",
      "negative_day_positive_sentiment=242639\n",
      "negative_day_negative_sentiment=209308\n",
      "negative_day_neutral_sentiment=262262\n"
     ]
    }
   ],
   "source": [
    "positive_day_positive_sentiment = 0\n",
    "positive_day_negative_sentiment = 0\n",
    "positive_day_neutral_sentiment = 0\n",
    "negative_day_positive_sentiment = 0\n",
    "negative_day_negative_sentiment = 0\n",
    "negative_day_neutral_sentiment = 0\n",
    "\n",
    "for comment in mongo_comments.find():\n",
    "    positive_day = comment[\"next_trading_day_positive\"]\n",
    "    sentiment = comment[\"sentiment_score\"][\"compound\"]\n",
    "    if positive_day:\n",
    "        if sentiment > 0:\n",
    "            positive_day_positive_sentiment += 1\n",
    "        elif sentiment < 0:\n",
    "            positive_day_negative_sentiment += 1\n",
    "        else:\n",
    "            positive_day_neutral_sentiment += 1\n",
    "    else:\n",
    "        if sentiment > 0:\n",
    "            negative_day_positive_sentiment += 1\n",
    "        elif sentiment < 0:\n",
    "            negative_day_negative_sentiment += 1\n",
    "        else:\n",
    "            negative_day_neutral_sentiment += 1\n",
    "print(f\"{positive_day_positive_sentiment=}\")\n",
    "print(f\"{positive_day_negative_sentiment=}\")\n",
    "print(f\"{positive_day_neutral_sentiment=}\")\n",
    "print(f\"{negative_day_positive_sentiment=}\")\n",
    "print(f\"{negative_day_negative_sentiment=}\")\n",
    "print(f\"{negative_day_neutral_sentiment=}\")\n",
    "\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive comments on positive days: 0.5383298462760289\n",
      "negative comments on positive days: 0.4616701537239712\n",
      "positive comments on negative days: 0.5368748990478972\n",
      "negative comments on negative days: 0.4631251009521028\n"
     ]
    }
   ],
   "source": [
    "print(f\"positive comments on positive days: {positive_day_positive_sentiment/(positive_day_positive_sentiment + positive_day_negative_sentiment)}\")\n",
    "print(f\"negative comments on positive days: {positive_day_negative_sentiment/(positive_day_positive_sentiment + positive_day_negative_sentiment)}\")\n",
    "print(f\"positive comments on negative days: {negative_day_positive_sentiment/(negative_day_positive_sentiment+negative_day_negative_sentiment)}\")\n",
    "print(f\"negative comments on negative days: {negative_day_negative_sentiment/(negative_day_positive_sentiment+negative_day_negative_sentiment)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## second analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = mongo_comments.distinct(\"next_trading_day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### insert result into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 0/374, positive_day=True, total sentiment: 10.315999999999999, combined text sentiment. {'neg': 0.129, 'neu': 0.691, 'pos': 0.179, 'compound': 0.9993}\n",
      "Day 1/374, positive_day=False, total sentiment: 31.199599999999855, combined text sentiment. {'neg': 0.145, 'neu': 0.708, 'pos': 0.147, 'compound': 0.9747}\n",
      "Day 2/374, positive_day=True, total sentiment: 31.273799999999973, combined text sentiment. {'neg': 0.128, 'neu': 0.722, 'pos': 0.15, 'compound': 1.0}\n",
      "Day 3/374, positive_day=True, total sentiment: 59.2673999999999, combined text sentiment. {'neg': 0.123, 'neu': 0.73, 'pos': 0.147, 'compound': 1.0}\n",
      "Day 4/374, positive_day=True, total sentiment: 57.59929999999994, combined text sentiment. {'neg': 0.114, 'neu': 0.731, 'pos': 0.155, 'compound': 1.0}\n",
      "Day 5/374, positive_day=True, total sentiment: 67.39569999999985, combined text sentiment. {'neg': 0.123, 'neu': 0.719, 'pos': 0.158, 'compound': 1.0}\n",
      "Day 6/374, positive_day=True, total sentiment: 120.85249999999989, combined text sentiment. {'neg': 0.126, 'neu': 0.715, 'pos': 0.159, 'compound': 1.0}\n",
      "Day 7/374, positive_day=True, total sentiment: 89.49429999999973, combined text sentiment. {'neg': 0.138, 'neu': 0.705, 'pos': 0.157, 'compound': 1.0}\n",
      "Day 8/374, positive_day=False, total sentiment: 106.94959999999982, combined text sentiment. {'neg': 0.131, 'neu': 0.714, 'pos': 0.155, 'compound': 1.0}\n",
      "Day 9/374, positive_day=True, total sentiment: 29.62620000000003, combined text sentiment. {'neg': 0.146, 'neu': 0.699, 'pos': 0.156, 'compound': 0.9998}\n",
      "Day 10/374, positive_day=True, total sentiment: 57.323699999999896, combined text sentiment. {'neg': 0.134, 'neu': 0.704, 'pos': 0.162, 'compound': 1.0}\n",
      "Day 11/374, positive_day=True, total sentiment: 82.3787, combined text sentiment. {'neg': 0.114, 'neu': 0.741, 'pos': 0.145, 'compound': 1.0}\n",
      "Day 12/374, positive_day=True, total sentiment: 81.5372, combined text sentiment. {'neg': 0.128, 'neu': 0.711, 'pos': 0.161, 'compound': 1.0}\n",
      "Day 13/374, positive_day=False, total sentiment: 107.81119999999962, combined text sentiment. {'neg': 0.086, 'neu': 0.79, 'pos': 0.124, 'compound': 1.0}\n",
      "Day 14/374, positive_day=True, total sentiment: 38.15389999999989, combined text sentiment. {'neg': 0.134, 'neu': 0.714, 'pos': 0.152, 'compound': 0.9999}\n",
      "Day 15/374, positive_day=True, total sentiment: 55.826600000000035, combined text sentiment. {'neg': 0.138, 'neu': 0.699, 'pos': 0.163, 'compound': 1.0}\n",
      "Day 16/374, positive_day=True, total sentiment: 64.4719999999999, combined text sentiment. {'neg': 0.117, 'neu': 0.748, 'pos': 0.134, 'compound': 0.9999}\n",
      "Day 17/374, positive_day=False, total sentiment: 4.260400000000045, combined text sentiment. {'neg': 0.139, 'neu': 0.711, 'pos': 0.15, 'compound': 0.9998}\n",
      "Day 18/374, positive_day=False, total sentiment: 33.400499999999965, combined text sentiment. {'neg': 0.124, 'neu': 0.747, 'pos': 0.128, 'compound': 0.9984}\n",
      "Day 19/374, positive_day=True, total sentiment: 38.07130000000005, combined text sentiment. {'neg': 0.145, 'neu': 0.702, 'pos': 0.154, 'compound': 0.9998}\n",
      "Day 20/374, positive_day=True, total sentiment: 92.02359999999985, combined text sentiment. {'neg': 0.136, 'neu': 0.696, 'pos': 0.167, 'compound': 1.0}\n",
      "Day 21/374, positive_day=True, total sentiment: 113.16859999999994, combined text sentiment. {'neg': 0.128, 'neu': 0.71, 'pos': 0.162, 'compound': 1.0}\n",
      "Day 22/374, positive_day=True, total sentiment: 106.8941999999999, combined text sentiment. {'neg': 0.121, 'neu': 0.707, 'pos': 0.172, 'compound': 1.0}\n",
      "Day 23/374, positive_day=True, total sentiment: 86.56039999999969, combined text sentiment. {'neg': 0.119, 'neu': 0.737, 'pos': 0.145, 'compound': 0.9999}\n",
      "Day 24/374, positive_day=False, total sentiment: 110.98569999999981, combined text sentiment. {'neg': 0.117, 'neu': 0.73, 'pos': 0.152, 'compound': 1.0}\n",
      "Day 25/374, positive_day=False, total sentiment: 125.67649999999956, combined text sentiment. {'neg': 0.132, 'neu': 0.697, 'pos': 0.171, 'compound': 1.0}\n",
      "Day 26/374, positive_day=True, total sentiment: 117.81319999999987, combined text sentiment. {'neg': 0.127, 'neu': 0.715, 'pos': 0.158, 'compound': 1.0}\n",
      "Day 27/374, positive_day=True, total sentiment: 14.73499999999997, combined text sentiment. {'neg': 0.144, 'neu': 0.697, 'pos': 0.159, 'compound': 0.9999}\n",
      "Day 28/374, positive_day=True, total sentiment: 66.88259999999985, combined text sentiment. {'neg': 0.135, 'neu': 0.706, 'pos': 0.158, 'compound': 0.9999}\n",
      "Day 29/374, positive_day=True, total sentiment: 105.42829999999984, combined text sentiment. {'neg': 0.149, 'neu': 0.702, 'pos': 0.15, 'compound': -0.994}\n",
      "Day 30/374, positive_day=False, total sentiment: 82.70489999999997, combined text sentiment. {'neg': 0.108, 'neu': 0.759, 'pos': 0.134, 'compound': 1.0}\n",
      "Day 31/374, positive_day=True, total sentiment: 113.59519999999969, combined text sentiment. {'neg': 0.091, 'neu': 0.798, 'pos': 0.111, 'compound': 0.9999}\n",
      "Day 32/374, positive_day=True, total sentiment: 133.26499999999976, combined text sentiment. {'neg': 0.103, 'neu': 0.764, 'pos': 0.133, 'compound': 1.0}\n",
      "Day 33/374, positive_day=True, total sentiment: 57.07879999999999, combined text sentiment. {'neg': 0.101, 'neu': 0.78, 'pos': 0.118, 'compound': 0.9999}\n",
      "Day 34/374, positive_day=False, total sentiment: 126.8108999999999, combined text sentiment. {'neg': 0.121, 'neu': 0.717, 'pos': 0.163, 'compound': 1.0}\n",
      "Day 35/374, positive_day=True, total sentiment: 114.63719999999982, combined text sentiment. {'neg': 0.117, 'neu': 0.721, 'pos': 0.162, 'compound': 1.0}\n",
      "Day 36/374, positive_day=True, total sentiment: 88.85609999999984, combined text sentiment. {'neg': 0.118, 'neu': 0.713, 'pos': 0.169, 'compound': 1.0}\n",
      "Day 37/374, positive_day=False, total sentiment: 98.87099999999971, combined text sentiment. {'neg': 0.123, 'neu': 0.713, 'pos': 0.164, 'compound': 1.0}\n",
      "Day 38/374, positive_day=False, total sentiment: 97.4059999999998, combined text sentiment. {'neg': 0.083, 'neu': 0.81, 'pos': 0.107, 'compound': 0.9999}\n",
      "Day 39/374, positive_day=False, total sentiment: 129.12779999999978, combined text sentiment. {'neg': 0.101, 'neu': 0.769, 'pos': 0.13, 'compound': 1.0}\n",
      "Day 40/374, positive_day=True, total sentiment: 67.40110000000004, combined text sentiment. {'neg': 0.125, 'neu': 0.705, 'pos': 0.17, 'compound': 1.0}\n",
      "Day 41/374, positive_day=False, total sentiment: 70.83259999999994, combined text sentiment. {'neg': 0.142, 'neu': 0.708, 'pos': 0.15, 'compound': 0.9983}\n",
      "Day 42/374, positive_day=False, total sentiment: 79.72460000000002, combined text sentiment. {'neg': 0.132, 'neu': 0.711, 'pos': 0.157, 'compound': 1.0}\n",
      "Day 43/374, positive_day=False, total sentiment: 122.93269999999985, combined text sentiment. {'neg': 0.118, 'neu': 0.719, 'pos': 0.163, 'compound': 1.0}\n",
      "Day 44/374, positive_day=False, total sentiment: 79.0828999999999, combined text sentiment. {'neg': 0.133, 'neu': 0.706, 'pos': 0.16, 'compound': 1.0}\n",
      "Day 45/374, positive_day=False, total sentiment: 49.58470000000001, combined text sentiment. {'neg': 0.134, 'neu': 0.705, 'pos': 0.161, 'compound': 1.0}\n",
      "Day 46/374, positive_day=True, total sentiment: 74.24929999999983, combined text sentiment. {'neg': 0.122, 'neu': 0.715, 'pos': 0.163, 'compound': 1.0}\n",
      "Day 47/374, positive_day=True, total sentiment: 69.51839999999996, combined text sentiment. {'neg': 0.127, 'neu': 0.712, 'pos': 0.161, 'compound': 1.0}\n",
      "Day 48/374, positive_day=True, total sentiment: 84.5340999999999, combined text sentiment. {'neg': 0.074, 'neu': 0.812, 'pos': 0.114, 'compound': 1.0}\n",
      "Day 49/374, positive_day=False, total sentiment: 80.4880999999999, combined text sentiment. {'neg': 0.129, 'neu': 0.706, 'pos': 0.165, 'compound': 1.0}\n",
      "Day 50/374, positive_day=True, total sentiment: 108.11929999999987, combined text sentiment. {'neg': 0.125, 'neu': 0.709, 'pos': 0.166, 'compound': 1.0}\n",
      "Day 51/374, positive_day=True, total sentiment: 89.83809999999995, combined text sentiment. {'neg': 0.125, 'neu': 0.704, 'pos': 0.171, 'compound': 1.0}\n",
      "Day 52/374, positive_day=True, total sentiment: 100.4361, combined text sentiment. {'neg': 0.116, 'neu': 0.718, 'pos': 0.166, 'compound': 1.0}\n",
      "Day 53/374, positive_day=False, total sentiment: 114.58539999999992, combined text sentiment. {'neg': 0.078, 'neu': 0.804, 'pos': 0.119, 'compound': 1.0}\n",
      "Day 54/374, positive_day=True, total sentiment: 99.45929999999993, combined text sentiment. {'neg': 0.113, 'neu': 0.74, 'pos': 0.147, 'compound': 1.0}\n",
      "Day 55/374, positive_day=False, total sentiment: 128.5952999999996, combined text sentiment. {'neg': 0.116, 'neu': 0.708, 'pos': 0.176, 'compound': 1.0}\n",
      "Day 56/374, positive_day=False, total sentiment: 104.10819999999984, combined text sentiment. {'neg': 0.127, 'neu': 0.716, 'pos': 0.157, 'compound': 1.0}\n",
      "Day 57/374, positive_day=True, total sentiment: 46.86019999999999, combined text sentiment. {'neg': 0.127, 'neu': 0.72, 'pos': 0.154, 'compound': 1.0}\n",
      "Day 58/374, positive_day=False, total sentiment: 89.28349999999986, combined text sentiment. {'neg': 0.127, 'neu': 0.71, 'pos': 0.163, 'compound': 1.0}\n",
      "Day 59/374, positive_day=True, total sentiment: 114.48749999999987, combined text sentiment. {'neg': 0.122, 'neu': 0.717, 'pos': 0.161, 'compound': 1.0}\n",
      "Day 60/374, positive_day=True, total sentiment: 73.30529999999992, combined text sentiment. {'neg': 0.133, 'neu': 0.717, 'pos': 0.15, 'compound': 0.9999}\n",
      "Day 61/374, positive_day=True, total sentiment: 112.8371999999999, combined text sentiment. {'neg': 0.119, 'neu': 0.718, 'pos': 0.162, 'compound': 1.0}\n",
      "Day 62/374, positive_day=True, total sentiment: 82.86759999999991, combined text sentiment. {'neg': 0.119, 'neu': 0.709, 'pos': 0.172, 'compound': 1.0}\n",
      "Day 63/374, positive_day=True, total sentiment: 102.80949999999987, combined text sentiment. {'neg': 0.119, 'neu': 0.72, 'pos': 0.161, 'compound': 1.0}\n",
      "Day 64/374, positive_day=True, total sentiment: 120.9222, combined text sentiment. {'neg': 0.13, 'neu': 0.701, 'pos': 0.168, 'compound': 1.0}\n",
      "Day 65/374, positive_day=True, total sentiment: 79.63019999999989, combined text sentiment. {'neg': 0.128, 'neu': 0.711, 'pos': 0.161, 'compound': 1.0}\n",
      "Day 66/374, positive_day=True, total sentiment: 131.03289999999984, combined text sentiment. {'neg': 0.119, 'neu': 0.714, 'pos': 0.167, 'compound': 1.0}\n",
      "Day 67/374, positive_day=False, total sentiment: 63.14439999999996, combined text sentiment. {'neg': 0.112, 'neu': 0.712, 'pos': 0.176, 'compound': 1.0}\n",
      "Day 68/374, positive_day=True, total sentiment: 114.63189999999997, combined text sentiment. {'neg': 0.108, 'neu': 0.716, 'pos': 0.176, 'compound': 1.0}\n",
      "Day 69/374, positive_day=False, total sentiment: 83.85039999999998, combined text sentiment. {'neg': 0.124, 'neu': 0.718, 'pos': 0.158, 'compound': 1.0}\n",
      "Day 70/374, positive_day=True, total sentiment: 157.17440000000025, combined text sentiment. {'neg': 0.102, 'neu': 0.733, 'pos': 0.165, 'compound': 1.0}\n",
      "Day 71/374, positive_day=False, total sentiment: 148.96329999999992, combined text sentiment. {'neg': 0.119, 'neu': 0.714, 'pos': 0.167, 'compound': 1.0}\n",
      "Day 72/374, positive_day=True, total sentiment: 173.5935999999999, combined text sentiment. {'neg': 0.082, 'neu': 0.782, 'pos': 0.137, 'compound': 1.0}\n",
      "Day 73/374, positive_day=False, total sentiment: 150.3725000000002, combined text sentiment. {'neg': 0.124, 'neu': 0.704, 'pos': 0.173, 'compound': 1.0}\n",
      "Day 74/374, positive_day=True, total sentiment: 199.10550000000023, combined text sentiment. {'neg': 0.09, 'neu': 0.766, 'pos': 0.144, 'compound': 1.0}\n",
      "Day 75/374, positive_day=True, total sentiment: 95.31359999999992, combined text sentiment. {'neg': 0.117, 'neu': 0.705, 'pos': 0.178, 'compound': 1.0}\n",
      "Day 76/374, positive_day=True, total sentiment: 86.33459999999991, combined text sentiment. {'neg': 0.114, 'neu': 0.724, 'pos': 0.162, 'compound': 1.0}\n",
      "Day 77/374, positive_day=False, total sentiment: 115.52719999999984, combined text sentiment. {'neg': 0.083, 'neu': 0.799, 'pos': 0.118, 'compound': 1.0}\n",
      "Day 78/374, positive_day=False, total sentiment: 161.6711000000001, combined text sentiment. {'neg': 0.128, 'neu': 0.7, 'pos': 0.172, 'compound': 1.0}\n",
      "Day 79/374, positive_day=True, total sentiment: 206.76380000000012, combined text sentiment. {'neg': 0.113, 'neu': 0.714, 'pos': 0.174, 'compound': 1.0}\n",
      "Day 80/374, positive_day=True, total sentiment: 159.73369999999997, combined text sentiment. {'neg': 0.106, 'neu': 0.72, 'pos': 0.175, 'compound': 1.0}\n",
      "Day 81/374, positive_day=True, total sentiment: 141.55660000000006, combined text sentiment. {'neg': 0.104, 'neu': 0.732, 'pos': 0.164, 'compound': 1.0}\n",
      "Day 82/374, positive_day=False, total sentiment: 163.0571000000001, combined text sentiment. {'neg': 0.116, 'neu': 0.708, 'pos': 0.176, 'compound': 1.0}\n",
      "Day 83/374, positive_day=False, total sentiment: 191.64769999999996, combined text sentiment. {'neg': 0.115, 'neu': 0.714, 'pos': 0.171, 'compound': 1.0}\n",
      "Day 84/374, positive_day=True, total sentiment: 58.277199999999965, combined text sentiment. {'neg': 0.13, 'neu': 0.715, 'pos': 0.155, 'compound': 1.0}\n",
      "Day 85/374, positive_day=False, total sentiment: 183.85789999999986, combined text sentiment. {'neg': 0.113, 'neu': 0.715, 'pos': 0.173, 'compound': 1.0}\n",
      "Day 86/374, positive_day=False, total sentiment: 68.6172999999999, combined text sentiment. {'neg': 0.148, 'neu': 0.702, 'pos': 0.151, 'compound': 0.9912}\n",
      "Day 87/374, positive_day=False, total sentiment: 81.74549999999972, combined text sentiment. {'neg': 0.137, 'neu': 0.705, 'pos': 0.158, 'compound': 1.0}\n",
      "Day 88/374, positive_day=False, total sentiment: 138.32749999999996, combined text sentiment. {'neg': 0.082, 'neu': 0.812, 'pos': 0.106, 'compound': 1.0}\n",
      "Day 89/374, positive_day=True, total sentiment: 124.19359999999949, combined text sentiment. {'neg': 0.115, 'neu': 0.76, 'pos': 0.125, 'compound': 0.9999}\n",
      "Day 90/374, positive_day=False, total sentiment: 102.43989999999992, combined text sentiment. {'neg': 0.102, 'neu': 0.781, 'pos': 0.117, 'compound': 1.0}\n",
      "Day 91/374, positive_day=True, total sentiment: 82.78399999999984, combined text sentiment. {'neg': 0.133, 'neu': 0.712, 'pos': 0.155, 'compound': 1.0}\n",
      "Day 92/374, positive_day=True, total sentiment: 89.81589999999979, combined text sentiment. {'neg': 0.133, 'neu': 0.705, 'pos': 0.162, 'compound': 1.0}\n",
      "Day 93/374, positive_day=True, total sentiment: 82.0281, combined text sentiment. {'neg': 0.131, 'neu': 0.701, 'pos': 0.168, 'compound': 1.0}\n",
      "Day 94/374, positive_day=False, total sentiment: 120.65329999999982, combined text sentiment. {'neg': 0.084, 'neu': 0.797, 'pos': 0.119, 'compound': 1.0}\n",
      "Day 95/374, positive_day=False, total sentiment: 67.3395999999998, combined text sentiment. {'neg': 0.132, 'neu': 0.706, 'pos': 0.162, 'compound': 1.0}\n",
      "Day 96/374, positive_day=True, total sentiment: 23.805500000000006, combined text sentiment. {'neg': 0.155, 'neu': 0.703, 'pos': 0.141, 'compound': -0.9999}\n",
      "Day 97/374, positive_day=False, total sentiment: 68.97019999999993, combined text sentiment. {'neg': 0.102, 'neu': 0.773, 'pos': 0.125, 'compound': 0.9999}\n",
      "Day 98/374, positive_day=False, total sentiment: 37.13869999999986, combined text sentiment. {'neg': 0.11, 'neu': 0.756, 'pos': 0.134, 'compound': 1.0}\n",
      "Day 99/374, positive_day=True, total sentiment: 48.9330999999999, combined text sentiment. {'neg': 0.142, 'neu': 0.723, 'pos': 0.135, 'compound': -0.9999}\n",
      "Day 100/374, positive_day=False, total sentiment: 42.7937999999999, combined text sentiment. {'neg': 0.144, 'neu': 0.705, 'pos': 0.151, 'compound': 0.9998}\n",
      "Day 101/374, positive_day=False, total sentiment: 71.51339999999992, combined text sentiment. {'neg': 0.119, 'neu': 0.717, 'pos': 0.164, 'compound': 1.0}\n",
      "Day 102/374, positive_day=True, total sentiment: 34.818200000000004, combined text sentiment. {'neg': 0.127, 'neu': 0.711, 'pos': 0.162, 'compound': 1.0}\n",
      "Day 103/374, positive_day=False, total sentiment: 41.211699999999894, combined text sentiment. {'neg': 0.137, 'neu': 0.708, 'pos': 0.156, 'compound': 0.9999}\n",
      "Day 104/374, positive_day=False, total sentiment: -0.34249999999999925, combined text sentiment. {'neg': 0.142, 'neu': 0.715, 'pos': 0.143, 'compound': -0.9311}\n",
      "Day 105/374, positive_day=True, total sentiment: 73.40629999999993, combined text sentiment. {'neg': 0.142, 'neu': 0.718, 'pos': 0.139, 'compound': -0.9995}\n",
      "Day 106/374, positive_day=True, total sentiment: 32.1436, combined text sentiment. {'neg': 0.13, 'neu': 0.729, 'pos': 0.141, 'compound': 0.9998}\n",
      "Day 107/374, positive_day=True, total sentiment: 30.163499999999942, combined text sentiment. {'neg': 0.113, 'neu': 0.775, 'pos': 0.112, 'compound': -0.9997}\n",
      "Day 108/374, positive_day=True, total sentiment: 21.27600000000003, combined text sentiment. {'neg': 0.145, 'neu': 0.719, 'pos': 0.135, 'compound': -1.0}\n",
      "Day 109/374, positive_day=True, total sentiment: 126.25710000000015, combined text sentiment. {'neg': 0.122, 'neu': 0.723, 'pos': 0.155, 'compound': 1.0}\n",
      "Day 110/374, positive_day=False, total sentiment: -0.18330000000000002, combined text sentiment. {'neg': 0.149, 'neu': 0.648, 'pos': 0.204, 'compound': 0.2792}\n",
      "Day 111/374, positive_day=False, total sentiment: 67.86399999999993, combined text sentiment. {'neg': 0.127, 'neu': 0.71, 'pos': 0.163, 'compound': 1.0}\n",
      "Day 112/374, positive_day=True, total sentiment: 2.4682, combined text sentiment. {'neg': 0.113, 'neu': 0.678, 'pos': 0.209, 'compound': 0.957}\n",
      "Day 113/374, positive_day=False, total sentiment: 43.54249999999997, combined text sentiment. {'neg': 0.138, 'neu': 0.709, 'pos': 0.153, 'compound': 0.9999}\n",
      "Day 114/374, positive_day=True, total sentiment: 4.3155, combined text sentiment. {'neg': 0.115, 'neu': 0.758, 'pos': 0.128, 'compound': 0.9621}\n",
      "Day 115/374, positive_day=True, total sentiment: 75.99539999999995, combined text sentiment. {'neg': 0.126, 'neu': 0.713, 'pos': 0.161, 'compound': 1.0}\n",
      "Day 116/374, positive_day=True, total sentiment: 77.5616, combined text sentiment. {'neg': 0.135, 'neu': 0.712, 'pos': 0.154, 'compound': 1.0}\n",
      "Day 117/374, positive_day=True, total sentiment: 63.01029999999999, combined text sentiment. {'neg': 0.116, 'neu': 0.74, 'pos': 0.144, 'compound': 1.0}\n",
      "Day 118/374, positive_day=False, total sentiment: 98.46469999999971, combined text sentiment. {'neg': 0.109, 'neu': 0.78, 'pos': 0.111, 'compound': -0.9893}\n",
      "Day 119/374, positive_day=False, total sentiment: 16.91999999999999, combined text sentiment. {'neg': 0.132, 'neu': 0.708, 'pos': 0.16, 'compound': 1.0}\n",
      "Day 120/374, positive_day=False, total sentiment: 94.8031, combined text sentiment. {'neg': 0.113, 'neu': 0.72, 'pos': 0.167, 'compound': 1.0}\n",
      "Day 121/374, positive_day=False, total sentiment: 91.11019999999986, combined text sentiment. {'neg': 0.119, 'neu': 0.729, 'pos': 0.151, 'compound': 1.0}\n",
      "Day 122/374, positive_day=True, total sentiment: 64.77339999999992, combined text sentiment. {'neg': 0.137, 'neu': 0.703, 'pos': 0.16, 'compound': 1.0}\n",
      "Day 123/374, positive_day=True, total sentiment: 90.8743999999997, combined text sentiment. {'neg': 0.098, 'neu': 0.784, 'pos': 0.118, 'compound': 1.0}\n",
      "Day 124/374, positive_day=True, total sentiment: 92.67009999999961, combined text sentiment. {'neg': 0.151, 'neu': 0.694, 'pos': 0.155, 'compound': 0.9994}\n",
      "Day 125/374, positive_day=True, total sentiment: 119.74919999999963, combined text sentiment. {'neg': 0.097, 'neu': 0.758, 'pos': 0.145, 'compound': 1.0}\n",
      "Day 126/374, positive_day=True, total sentiment: 41.146699999999925, combined text sentiment. {'neg': 0.124, 'neu': 0.724, 'pos': 0.152, 'compound': 1.0}\n",
      "Day 127/374, positive_day=False, total sentiment: 151.51039999999958, combined text sentiment. {'neg': 0.104, 'neu': 0.765, 'pos': 0.132, 'compound': 1.0}\n",
      "Day 128/374, positive_day=False, total sentiment: 85.74399999999984, combined text sentiment. {'neg': 0.119, 'neu': 0.701, 'pos': 0.18, 'compound': 1.0}\n",
      "Day 129/374, positive_day=True, total sentiment: 40.40070000000002, combined text sentiment. {'neg': 0.14, 'neu': 0.703, 'pos': 0.157, 'compound': 0.9999}\n",
      "Day 130/374, positive_day=True, total sentiment: 42.130300000000005, combined text sentiment. {'neg': 0.131, 'neu': 0.707, 'pos': 0.162, 'compound': 1.0}\n",
      "Day 131/374, positive_day=True, total sentiment: 82.26149999999978, combined text sentiment. {'neg': 0.121, 'neu': 0.721, 'pos': 0.158, 'compound': 1.0}\n",
      "Day 132/374, positive_day=True, total sentiment: 73.56530000000012, combined text sentiment. {'neg': 0.124, 'neu': 0.715, 'pos': 0.161, 'compound': 1.0}\n",
      "Day 133/374, positive_day=True, total sentiment: 106.59619999999975, combined text sentiment. {'neg': 0.1, 'neu': 0.763, 'pos': 0.136, 'compound': 1.0}\n",
      "Day 134/374, positive_day=False, total sentiment: 76.08019999999998, combined text sentiment. {'neg': 0.135, 'neu': 0.694, 'pos': 0.172, 'compound': 1.0}\n",
      "Day 135/374, positive_day=False, total sentiment: 72.3813999999998, combined text sentiment. {'neg': 0.128, 'neu': 0.706, 'pos': 0.165, 'compound': 1.0}\n",
      "Day 136/374, positive_day=True, total sentiment: 123.27869999999969, combined text sentiment. {'neg': 0.123, 'neu': 0.713, 'pos': 0.164, 'compound': 1.0}\n",
      "Day 137/374, positive_day=False, total sentiment: 96.61159999999983, combined text sentiment. {'neg': 0.132, 'neu': 0.715, 'pos': 0.154, 'compound': 1.0}\n",
      "Day 138/374, positive_day=True, total sentiment: 126.07969999999969, combined text sentiment. {'neg': 0.117, 'neu': 0.709, 'pos': 0.174, 'compound': 1.0}\n",
      "Day 139/374, positive_day=True, total sentiment: 30.149800000000013, combined text sentiment. {'neg': 0.142, 'neu': 0.712, 'pos': 0.146, 'compound': -0.6832}\n",
      "Day 140/374, positive_day=True, total sentiment: 136.45889999999974, combined text sentiment. {'neg': 0.085, 'neu': 0.793, 'pos': 0.122, 'compound': 1.0}\n",
      "Day 141/374, positive_day=False, total sentiment: 120.2492999999997, combined text sentiment. {'neg': 0.125, 'neu': 0.71, 'pos': 0.165, 'compound': 1.0}\n",
      "Day 142/374, positive_day=True, total sentiment: 120.39989999999985, combined text sentiment. {'neg': 0.093, 'neu': 0.806, 'pos': 0.1, 'compound': 0.9998}\n",
      "Day 143/374, positive_day=False, total sentiment: 127.19369999999986, combined text sentiment. {'neg': 0.092, 'neu': 0.769, 'pos': 0.139, 'compound': 1.0}\n",
      "Day 144/374, positive_day=False, total sentiment: 70.44279999999996, combined text sentiment. {'neg': 0.128, 'neu': 0.709, 'pos': 0.163, 'compound': 1.0}\n",
      "Day 145/374, positive_day=False, total sentiment: 107.85119999999982, combined text sentiment. {'neg': 0.135, 'neu': 0.687, 'pos': 0.177, 'compound': 1.0}\n",
      "Day 146/374, positive_day=False, total sentiment: 119.10329999999956, combined text sentiment. {'neg': 0.096, 'neu': 0.78, 'pos': 0.123, 'compound': 1.0}\n",
      "Day 147/374, positive_day=False, total sentiment: 91.90779999999977, combined text sentiment. {'neg': 0.108, 'neu': 0.782, 'pos': 0.11, 'compound': 0.9901}\n",
      "Day 148/374, positive_day=False, total sentiment: 104.71759999999985, combined text sentiment. {'neg': 0.106, 'neu': 0.768, 'pos': 0.126, 'compound': 1.0}\n",
      "Day 149/374, positive_day=True, total sentiment: 24.10679999999999, combined text sentiment. {'neg': 0.134, 'neu': 0.734, 'pos': 0.132, 'compound': -0.9999}\n",
      "Day 150/374, positive_day=True, total sentiment: 20.397499999999873, combined text sentiment. {'neg': 0.131, 'neu': 0.735, 'pos': 0.134, 'compound': -0.9989}\n",
      "Day 151/374, positive_day=True, total sentiment: 87.91039999999978, combined text sentiment. {'neg': 0.133, 'neu': 0.704, 'pos': 0.162, 'compound': 1.0}\n",
      "Day 152/374, positive_day=False, total sentiment: 105.22859999999991, combined text sentiment. {'neg': 0.135, 'neu': 0.703, 'pos': 0.162, 'compound': 1.0}\n",
      "Day 153/374, positive_day=False, total sentiment: 72.38059999999992, combined text sentiment. {'neg': 0.136, 'neu': 0.699, 'pos': 0.165, 'compound': 1.0}\n",
      "Day 154/374, positive_day=True, total sentiment: 59.74469999999998, combined text sentiment. {'neg': 0.1, 'neu': 0.787, 'pos': 0.112, 'compound': 0.9999}\n",
      "Day 155/374, positive_day=False, total sentiment: 131.88089999999983, combined text sentiment. {'neg': 0.134, 'neu': 0.734, 'pos': 0.132, 'compound': -0.9997}\n",
      "Day 156/374, positive_day=True, total sentiment: 91.69059999999962, combined text sentiment. {'neg': 0.121, 'neu': 0.749, 'pos': 0.13, 'compound': 0.9998}\n",
      "Day 157/374, positive_day=True, total sentiment: 70.40839999999987, combined text sentiment. {'neg': 0.096, 'neu': 0.801, 'pos': 0.103, 'compound': 0.9995}\n",
      "Day 158/374, positive_day=True, total sentiment: 113.78499999999956, combined text sentiment. {'neg': 0.137, 'neu': 0.703, 'pos': 0.16, 'compound': 1.0}\n",
      "Day 159/374, positive_day=False, total sentiment: 75.51849999999979, combined text sentiment. {'neg': 0.128, 'neu': 0.709, 'pos': 0.163, 'compound': 1.0}\n",
      "Day 160/374, positive_day=True, total sentiment: 57.87529999999987, combined text sentiment. {'neg': 0.15, 'neu': 0.707, 'pos': 0.143, 'compound': -0.9999}\n",
      "Day 161/374, positive_day=False, total sentiment: 152.74720000000028, combined text sentiment. {'neg': 0.097, 'neu': 0.772, 'pos': 0.131, 'compound': 1.0}\n",
      "Day 162/374, positive_day=False, total sentiment: 178.1777000000001, combined text sentiment. {'neg': 0.121, 'neu': 0.703, 'pos': 0.175, 'compound': 1.0}\n",
      "Day 163/374, positive_day=True, total sentiment: 166.30230000000012, combined text sentiment. {'neg': 0.096, 'neu': 0.784, 'pos': 0.119, 'compound': 1.0}\n",
      "Day 164/374, positive_day=False, total sentiment: 61.06929999999995, combined text sentiment. {'neg': 0.16, 'neu': 0.695, 'pos': 0.145, 'compound': -1.0}\n",
      "Day 165/374, positive_day=True, total sentiment: 126.20759999999981, combined text sentiment. {'neg': 0.138, 'neu': 0.7, 'pos': 0.161, 'compound': 1.0}\n",
      "Day 166/374, positive_day=True, total sentiment: 169.23350000000042, combined text sentiment. {'neg': 0.118, 'neu': 0.741, 'pos': 0.141, 'compound': 1.0}\n",
      "Day 167/374, positive_day=False, total sentiment: 35.29879999999993, combined text sentiment. {'neg': 0.144, 'neu': 0.713, 'pos': 0.144, 'compound': -0.9999}\n",
      "Day 168/374, positive_day=False, total sentiment: 145.30390000000014, combined text sentiment. {'neg': 0.123, 'neu': 0.733, 'pos': 0.144, 'compound': 1.0}\n",
      "Day 169/374, positive_day=True, total sentiment: 59.12589999999989, combined text sentiment. {'neg': 0.13, 'neu': 0.708, 'pos': 0.161, 'compound': 1.0}\n",
      "Day 170/374, positive_day=True, total sentiment: 87.63939999999981, combined text sentiment. {'neg': 0.097, 'neu': 0.784, 'pos': 0.118, 'compound': 1.0}\n",
      "Day 171/374, positive_day=True, total sentiment: 103.40839999999969, combined text sentiment. {'neg': 0.12, 'neu': 0.746, 'pos': 0.135, 'compound': 1.0}\n",
      "Day 172/374, positive_day=True, total sentiment: 33.715399999999974, combined text sentiment. {'neg': 0.139, 'neu': 0.716, 'pos': 0.146, 'compound': 0.9991}\n",
      "Day 173/374, positive_day=False, total sentiment: 44.68439999999998, combined text sentiment. {'neg': 0.146, 'neu': 0.706, 'pos': 0.148, 'compound': -0.9697}\n",
      "Day 174/374, positive_day=True, total sentiment: 124.8632999999996, combined text sentiment. {'neg': 0.125, 'neu': 0.711, 'pos': 0.164, 'compound': 1.0}\n",
      "Day 175/374, positive_day=True, total sentiment: 126.13139999999986, combined text sentiment. {'neg': 0.131, 'neu': 0.709, 'pos': 0.16, 'compound': 1.0}\n",
      "Day 176/374, positive_day=False, total sentiment: 50.766900000000064, combined text sentiment. {'neg': 0.146, 'neu': 0.696, 'pos': 0.159, 'compound': 1.0}\n",
      "Day 177/374, positive_day=False, total sentiment: 122.75949999999945, combined text sentiment. {'neg': 0.12, 'neu': 0.751, 'pos': 0.129, 'compound': 0.9997}\n",
      "Day 178/374, positive_day=True, total sentiment: 5.393800000000026, combined text sentiment. {'neg': 0.147, 'neu': 0.689, 'pos': 0.164, 'compound': 1.0}\n",
      "Day 179/374, positive_day=True, total sentiment: 99.1144999999997, combined text sentiment. {'neg': 0.121, 'neu': 0.743, 'pos': 0.136, 'compound': 1.0}\n",
      "Day 180/374, positive_day=False, total sentiment: 72.76599999999999, combined text sentiment. {'neg': 0.119, 'neu': 0.731, 'pos': 0.15, 'compound': 1.0}\n",
      "Day 181/374, positive_day=False, total sentiment: 66.0335999999999, combined text sentiment. {'neg': 0.119, 'neu': 0.746, 'pos': 0.135, 'compound': 0.9999}\n",
      "Day 182/374, positive_day=False, total sentiment: 63.04899999999987, combined text sentiment. {'neg': 0.116, 'neu': 0.752, 'pos': 0.132, 'compound': 0.9998}\n",
      "Day 183/374, positive_day=False, total sentiment: 100.9481999999998, combined text sentiment. {'neg': 0.092, 'neu': 0.792, 'pos': 0.116, 'compound': 0.9999}\n",
      "Day 184/374, positive_day=True, total sentiment: 95.93669999999987, combined text sentiment. {'neg': 0.089, 'neu': 0.797, 'pos': 0.115, 'compound': 1.0}\n",
      "Day 185/374, positive_day=False, total sentiment: 52.35109999999976, combined text sentiment. {'neg': 0.124, 'neu': 0.732, 'pos': 0.144, 'compound': 1.0}\n",
      "Day 186/374, positive_day=False, total sentiment: 118.66109999999966, combined text sentiment. {'neg': 0.093, 'neu': 0.789, 'pos': 0.117, 'compound': 1.0}\n",
      "Day 187/374, positive_day=True, total sentiment: 68.43780000000002, combined text sentiment. {'neg': 0.14, 'neu': 0.694, 'pos': 0.167, 'compound': 1.0}\n",
      "Day 188/374, positive_day=False, total sentiment: 20.812900000000027, combined text sentiment. {'neg': 0.132, 'neu': 0.709, 'pos': 0.159, 'compound': 1.0}\n",
      "Day 189/374, positive_day=False, total sentiment: 41.1211, combined text sentiment. {'neg': 0.118, 'neu': 0.756, 'pos': 0.126, 'compound': 0.9995}\n",
      "Day 190/374, positive_day=True, total sentiment: 37.604299999999895, combined text sentiment. {'neg': 0.143, 'neu': 0.705, 'pos': 0.152, 'compound': 0.9999}\n",
      "Day 191/374, positive_day=True, total sentiment: 54.3543999999999, combined text sentiment. {'neg': 0.124, 'neu': 0.739, 'pos': 0.137, 'compound': 0.9999}\n",
      "Day 192/374, positive_day=False, total sentiment: 111.09439999999968, combined text sentiment. {'neg': 0.135, 'neu': 0.699, 'pos': 0.165, 'compound': 1.0}\n",
      "Day 193/374, positive_day=False, total sentiment: 10.788400000000014, combined text sentiment. {'neg': 0.152, 'neu': 0.71, 'pos': 0.138, 'compound': -1.0}\n",
      "Day 194/374, positive_day=True, total sentiment: 33.21169999999995, combined text sentiment. {'neg': 0.139, 'neu': 0.71, 'pos': 0.151, 'compound': 0.9999}\n",
      "Day 195/374, positive_day=True, total sentiment: 97.28379999999996, combined text sentiment. {'neg': 0.134, 'neu': 0.696, 'pos': 0.171, 'compound': 1.0}\n",
      "Day 196/374, positive_day=True, total sentiment: -24.114799999999967, combined text sentiment. {'neg': 0.139, 'neu': 0.725, 'pos': 0.136, 'compound': -0.9999}\n",
      "Day 197/374, positive_day=False, total sentiment: 118.7421999999997, combined text sentiment. {'neg': 0.116, 'neu': 0.756, 'pos': 0.128, 'compound': 0.9999}\n",
      "Day 198/374, positive_day=True, total sentiment: 96.20009999999981, combined text sentiment. {'neg': 0.134, 'neu': 0.712, 'pos': 0.155, 'compound': 1.0}\n",
      "Day 199/374, positive_day=False, total sentiment: 76.67099999999982, combined text sentiment. {'neg': 0.142, 'neu': 0.705, 'pos': 0.153, 'compound': 0.9998}\n",
      "Day 200/374, positive_day=True, total sentiment: 76.00089999999976, combined text sentiment. {'neg': 0.133, 'neu': 0.704, 'pos': 0.163, 'compound': 1.0}\n",
      "Day 201/374, positive_day=False, total sentiment: 97.44499999999971, combined text sentiment. {'neg': 0.098, 'neu': 0.786, 'pos': 0.116, 'compound': 0.9999}\n",
      "Day 202/374, positive_day=True, total sentiment: 26.1821, combined text sentiment. {'neg': 0.144, 'neu': 0.704, 'pos': 0.152, 'compound': 0.9992}\n",
      "Day 203/374, positive_day=False, total sentiment: 67.60489999999982, combined text sentiment. {'neg': 0.145, 'neu': 0.698, 'pos': 0.157, 'compound': 0.9999}\n",
      "Day 204/374, positive_day=True, total sentiment: 89.77499999999992, combined text sentiment. {'neg': 0.141, 'neu': 0.698, 'pos': 0.161, 'compound': 1.0}\n",
      "Day 205/374, positive_day=True, total sentiment: 161.49690000000004, combined text sentiment. {'neg': 0.129, 'neu': 0.703, 'pos': 0.167, 'compound': 1.0}\n",
      "Day 206/374, positive_day=True, total sentiment: 136.17879999999968, combined text sentiment. {'neg': 0.103, 'neu': 0.762, 'pos': 0.135, 'compound': 1.0}\n",
      "Day 207/374, positive_day=True, total sentiment: 83.94249999999965, combined text sentiment. {'neg': 0.134, 'neu': 0.708, 'pos': 0.158, 'compound': 1.0}\n",
      "Day 208/374, positive_day=False, total sentiment: 62.917799999999964, combined text sentiment. {'neg': 0.13, 'neu': 0.715, 'pos': 0.155, 'compound': 1.0}\n",
      "Day 209/374, positive_day=True, total sentiment: 99.67179999999986, combined text sentiment. {'neg': 0.113, 'neu': 0.741, 'pos': 0.146, 'compound': 1.0}\n",
      "Day 210/374, positive_day=False, total sentiment: 59.17409999999996, combined text sentiment. {'neg': 0.136, 'neu': 0.71, 'pos': 0.155, 'compound': 1.0}\n",
      "Day 211/374, positive_day=True, total sentiment: 92.10359999999982, combined text sentiment. {'neg': 0.138, 'neu': 0.702, 'pos': 0.16, 'compound': 1.0}\n",
      "Day 212/374, positive_day=True, total sentiment: 43.13579999999993, combined text sentiment. {'neg': 0.138, 'neu': 0.706, 'pos': 0.157, 'compound': 1.0}\n",
      "Day 213/374, positive_day=False, total sentiment: 44.54349999999974, combined text sentiment. {'neg': 0.144, 'neu': 0.71, 'pos': 0.147, 'compound': 0.9632}\n",
      "Day 214/374, positive_day=True, total sentiment: 84.86769999999997, combined text sentiment. {'neg': 0.103, 'neu': 0.788, 'pos': 0.108, 'compound': 0.9998}\n",
      "Day 215/374, positive_day=True, total sentiment: 107.76369999999982, combined text sentiment. {'neg': 0.141, 'neu': 0.701, 'pos': 0.158, 'compound': 1.0}\n",
      "Day 216/374, positive_day=True, total sentiment: 213.78310000000064, combined text sentiment. {'neg': 0.104, 'neu': 0.757, 'pos': 0.139, 'compound': 1.0}\n",
      "Day 217/374, positive_day=False, total sentiment: 121.26369999999966, combined text sentiment. {'neg': 0.127, 'neu': 0.721, 'pos': 0.152, 'compound': 1.0}\n",
      "Day 218/374, positive_day=True, total sentiment: 71.93439999999988, combined text sentiment. {'neg': 0.091, 'neu': 0.811, 'pos': 0.098, 'compound': 0.9945}\n",
      "Day 219/374, positive_day=True, total sentiment: 117.59719999999997, combined text sentiment. {'neg': 0.125, 'neu': 0.712, 'pos': 0.162, 'compound': 1.0}\n",
      "Day 220/374, positive_day=True, total sentiment: 135.32239999999985, combined text sentiment. {'neg': 0.121, 'neu': 0.73, 'pos': 0.149, 'compound': 1.0}\n",
      "Day 221/374, positive_day=True, total sentiment: 163.90400000000022, combined text sentiment. {'neg': 0.111, 'neu': 0.748, 'pos': 0.141, 'compound': 1.0}\n",
      "Day 222/374, positive_day=True, total sentiment: 126.3669999999993, combined text sentiment. {'neg': 0.114, 'neu': 0.744, 'pos': 0.143, 'compound': 1.0}\n",
      "Day 223/374, positive_day=False, total sentiment: 103.15709999999999, combined text sentiment. {'neg': 0.133, 'neu': 0.73, 'pos': 0.137, 'compound': 0.9998}\n",
      "Day 224/374, positive_day=False, total sentiment: 140.4525000000001, combined text sentiment. {'neg': 0.147, 'neu': 0.708, 'pos': 0.146, 'compound': -0.9965}\n",
      "Day 225/374, positive_day=False, total sentiment: 31.99270000000001, combined text sentiment. {'neg': 0.098, 'neu': 0.782, 'pos': 0.12, 'compound': 1.0}\n",
      "Day 226/374, positive_day=True, total sentiment: 98.90729999999996, combined text sentiment. {'neg': 0.131, 'neu': 0.698, 'pos': 0.171, 'compound': 1.0}\n",
      "Day 227/374, positive_day=True, total sentiment: 108.01829999999988, combined text sentiment. {'neg': 0.091, 'neu': 0.801, 'pos': 0.108, 'compound': 0.9999}\n",
      "Day 228/374, positive_day=True, total sentiment: 109.23959999999973, combined text sentiment. {'neg': 0.099, 'neu': 0.779, 'pos': 0.122, 'compound': 1.0}\n",
      "Day 229/374, positive_day=True, total sentiment: 88.3297999999999, combined text sentiment. {'neg': 0.144, 'neu': 0.696, 'pos': 0.16, 'compound': 1.0}\n",
      "Day 230/374, positive_day=False, total sentiment: 242.35850000000005, combined text sentiment. {'neg': 0.124, 'neu': 0.714, 'pos': 0.162, 'compound': 1.0}\n",
      "Day 231/374, positive_day=False, total sentiment: 56.625799999999984, combined text sentiment. {'neg': 0.133, 'neu': 0.707, 'pos': 0.16, 'compound': 1.0}\n",
      "Day 232/374, positive_day=False, total sentiment: 113.15189999999994, combined text sentiment. {'neg': 0.096, 'neu': 0.789, 'pos': 0.114, 'compound': 1.0}\n",
      "Day 233/374, positive_day=True, total sentiment: 110.74849999999964, combined text sentiment. {'neg': 0.125, 'neu': 0.732, 'pos': 0.144, 'compound': 1.0}\n",
      "Day 234/374, positive_day=True, total sentiment: 168.1512, combined text sentiment. {'neg': 0.107, 'neu': 0.743, 'pos': 0.15, 'compound': 1.0}\n",
      "Day 235/374, positive_day=True, total sentiment: 179.21049999999985, combined text sentiment. {'neg': 0.11, 'neu': 0.718, 'pos': 0.172, 'compound': 1.0}\n",
      "Day 236/374, positive_day=False, total sentiment: 105.10039999999974, combined text sentiment. {'neg': 0.125, 'neu': 0.711, 'pos': 0.164, 'compound': 1.0}\n",
      "Day 237/374, positive_day=False, total sentiment: 81.58329999999974, combined text sentiment. {'neg': 0.134, 'neu': 0.704, 'pos': 0.162, 'compound': 1.0}\n",
      "Day 238/374, positive_day=True, total sentiment: 59.60799999999993, combined text sentiment. {'neg': 0.141, 'neu': 0.714, 'pos': 0.145, 'compound': 0.9975}\n",
      "Day 239/374, positive_day=True, total sentiment: 100.52799999999992, combined text sentiment. {'neg': 0.121, 'neu': 0.726, 'pos': 0.153, 'compound': 1.0}\n",
      "Day 240/374, positive_day=True, total sentiment: 111.72739999999968, combined text sentiment. {'neg': 0.111, 'neu': 0.76, 'pos': 0.129, 'compound': 1.0}\n",
      "Day 241/374, positive_day=True, total sentiment: 120.46330000000003, combined text sentiment. {'neg': 0.11, 'neu': 0.757, 'pos': 0.133, 'compound': 1.0}\n",
      "Day 242/374, positive_day=True, total sentiment: 73.63369999999989, combined text sentiment. {'neg': 0.132, 'neu': 0.717, 'pos': 0.151, 'compound': 1.0}\n",
      "Day 243/374, positive_day=True, total sentiment: 128.5045999999996, combined text sentiment. {'neg': 0.124, 'neu': 0.714, 'pos': 0.162, 'compound': 1.0}\n",
      "Day 244/374, positive_day=True, total sentiment: 133.4138999999998, combined text sentiment. {'neg': 0.082, 'neu': 0.817, 'pos': 0.102, 'compound': 0.9999}\n",
      "Day 245/374, positive_day=True, total sentiment: 155.09159999999966, combined text sentiment. {'neg': 0.119, 'neu': 0.734, 'pos': 0.147, 'compound': 1.0}\n",
      "Day 246/374, positive_day=True, total sentiment: 80.9902, combined text sentiment. {'neg': 0.136, 'neu': 0.706, 'pos': 0.157, 'compound': 1.0}\n",
      "Day 247/374, positive_day=True, total sentiment: 71.37259999999992, combined text sentiment. {'neg': 0.125, 'neu': 0.723, 'pos': 0.153, 'compound': 1.0}\n",
      "Day 248/374, positive_day=True, total sentiment: 172.80479999999994, combined text sentiment. {'neg': 0.126, 'neu': 0.701, 'pos': 0.173, 'compound': 1.0}\n",
      "Day 249/374, positive_day=False, total sentiment: 97.95949999999976, combined text sentiment. {'neg': 0.117, 'neu': 0.731, 'pos': 0.152, 'compound': 1.0}\n",
      "Day 250/374, positive_day=False, total sentiment: 120.5774, combined text sentiment. {'neg': 0.096, 'neu': 0.761, 'pos': 0.143, 'compound': 1.0}\n",
      "Day 251/374, positive_day=True, total sentiment: 119.97509999999977, combined text sentiment. {'neg': 0.099, 'neu': 0.774, 'pos': 0.127, 'compound': 1.0}\n",
      "Day 252/374, positive_day=True, total sentiment: 185.2486999999998, combined text sentiment. {'neg': 0.128, 'neu': 0.706, 'pos': 0.166, 'compound': 1.0}\n",
      "Day 253/374, positive_day=False, total sentiment: 154.98230000000027, combined text sentiment. {'neg': 0.121, 'neu': 0.711, 'pos': 0.168, 'compound': 1.0}\n",
      "Day 254/374, positive_day=True, total sentiment: 13.015600000000063, combined text sentiment. {'neg': 0.131, 'neu': 0.736, 'pos': 0.132, 'compound': -0.9999}\n",
      "Day 255/374, positive_day=False, total sentiment: 142.20659999999972, combined text sentiment. {'neg': 0.098, 'neu': 0.802, 'pos': 0.099, 'compound': -0.9972}\n",
      "Day 256/374, positive_day=True, total sentiment: 103.97989999999982, combined text sentiment. {'neg': 0.101, 'neu': 0.799, 'pos': 0.1, 'compound': -0.9999}\n",
      "Day 257/374, positive_day=True, total sentiment: -301.39040000000006, combined text sentiment. {'neg': 0.168, 'neu': 0.689, 'pos': 0.142, 'compound': -1.0}\n",
      "Day 258/374, positive_day=False, total sentiment: 212.09880000000055, combined text sentiment. {'neg': 0.112, 'neu': 0.725, 'pos': 0.163, 'compound': 1.0}\n",
      "Day 259/374, positive_day=True, total sentiment: 179.72750000000016, combined text sentiment. {'neg': 0.133, 'neu': 0.707, 'pos': 0.161, 'compound': 1.0}\n",
      "Day 260/374, positive_day=False, total sentiment: 289.53319999999945, combined text sentiment. {'neg': 0.113, 'neu': 0.716, 'pos': 0.171, 'compound': 1.0}\n",
      "Day 261/374, positive_day=True, total sentiment: 273.5163000000006, combined text sentiment. {'neg': 0.113, 'neu': 0.744, 'pos': 0.143, 'compound': 1.0}\n",
      "Day 262/374, positive_day=True, total sentiment: 251.72630000000024, combined text sentiment. {'neg': 0.097, 'neu': 0.785, 'pos': 0.118, 'compound': 1.0}\n",
      "Day 263/374, positive_day=True, total sentiment: 277.9433000000006, combined text sentiment. {'neg': 0.115, 'neu': 0.721, 'pos': 0.164, 'compound': 1.0}\n",
      "Day 264/374, positive_day=False, total sentiment: 384.3625000000006, combined text sentiment. {'neg': 0.107, 'neu': 0.754, 'pos': 0.139, 'compound': 1.0}\n",
      "Day 265/374, positive_day=True, total sentiment: 144.17779999999985, combined text sentiment. {'neg': 0.105, 'neu': 0.76, 'pos': 0.134, 'compound': 1.0}\n",
      "Day 266/374, positive_day=True, total sentiment: 272.3668999999997, combined text sentiment. {'neg': 0.08, 'neu': 0.815, 'pos': 0.105, 'compound': 1.0}\n",
      "Day 267/374, positive_day=False, total sentiment: 205.75130000000001, combined text sentiment. {'neg': 0.139, 'neu': 0.708, 'pos': 0.154, 'compound': 1.0}\n",
      "Day 268/374, positive_day=False, total sentiment: 274.09299999999945, combined text sentiment. {'neg': 0.102, 'neu': 0.775, 'pos': 0.123, 'compound': 1.0}\n",
      "Day 269/374, positive_day=True, total sentiment: 98.4864999999999, combined text sentiment. {'neg': 0.127, 'neu': 0.742, 'pos': 0.131, 'compound': -0.8192}\n",
      "Day 270/374, positive_day=False, total sentiment: 233.09979999999942, combined text sentiment. {'neg': 0.116, 'neu': 0.762, 'pos': 0.122, 'compound': 0.9997}\n",
      "Day 271/374, positive_day=True, total sentiment: 301.7122999999994, combined text sentiment. {'neg': 0.117, 'neu': 0.736, 'pos': 0.148, 'compound': 1.0}\n",
      "Day 272/374, positive_day=False, total sentiment: 291.19160000000113, combined text sentiment. {'neg': 0.121, 'neu': 0.737, 'pos': 0.142, 'compound': 1.0}\n",
      "Day 273/374, positive_day=True, total sentiment: 486.64429999999936, combined text sentiment. {'neg': 0.121, 'neu': 0.725, 'pos': 0.154, 'compound': 1.0}\n",
      "Day 274/374, positive_day=True, total sentiment: 413.9041000000024, combined text sentiment. {'neg': 0.145, 'neu': 0.701, 'pos': 0.154, 'compound': 1.0}\n",
      "Day 275/374, positive_day=True, total sentiment: 522.5360000000002, combined text sentiment. {'neg': 0.126, 'neu': 0.722, 'pos': 0.152, 'compound': 1.0}\n",
      "Day 276/374, positive_day=True, total sentiment: 707.1741999999998, combined text sentiment. {'neg': 0.122, 'neu': 0.711, 'pos': 0.167, 'compound': 1.0}\n",
      "Day 277/374, positive_day=False, total sentiment: 357.0447999999997, combined text sentiment. {'neg': 0.128, 'neu': 0.717, 'pos': 0.155, 'compound': 1.0}\n",
      "Day 278/374, positive_day=True, total sentiment: 195.99849999999998, combined text sentiment. {'neg': 0.085, 'neu': 0.822, 'pos': 0.094, 'compound': 0.9999}\n",
      "Day 279/374, positive_day=True, total sentiment: 297.5731000000013, combined text sentiment. {'neg': 0.121, 'neu': 0.726, 'pos': 0.153, 'compound': 1.0}\n",
      "Day 280/374, positive_day=True, total sentiment: 591.8488, combined text sentiment. {'neg': 0.112, 'neu': 0.751, 'pos': 0.137, 'compound': 1.0}\n",
      "Day 281/374, positive_day=False, total sentiment: 509.63070000000073, combined text sentiment. {'neg': 0.104, 'neu': 0.757, 'pos': 0.138, 'compound': 1.0}\n",
      "Day 282/374, positive_day=True, total sentiment: 189.50379999999967, combined text sentiment. {'neg': 0.102, 'neu': 0.766, 'pos': 0.131, 'compound': 1.0}\n",
      "Day 283/374, positive_day=False, total sentiment: 471.22049999999933, combined text sentiment. {'neg': 0.127, 'neu': 0.725, 'pos': 0.148, 'compound': 1.0}\n",
      "Day 284/374, positive_day=True, total sentiment: 174.49640000000028, combined text sentiment. {'neg': 0.135, 'neu': 0.712, 'pos': 0.154, 'compound': 1.0}\n",
      "Day 285/374, positive_day=False, total sentiment: 561.7278999999997, combined text sentiment. {'neg': 0.129, 'neu': 0.72, 'pos': 0.151, 'compound': 1.0}\n",
      "Day 286/374, positive_day=False, total sentiment: 449.3356000000009, combined text sentiment. {'neg': 0.104, 'neu': 0.766, 'pos': 0.13, 'compound': 1.0}\n",
      "Day 287/374, positive_day=False, total sentiment: 253.63250000000164, combined text sentiment. {'neg': 0.123, 'neu': 0.734, 'pos': 0.143, 'compound': 1.0}\n",
      "Day 288/374, positive_day=False, total sentiment: 113.44409999999992, combined text sentiment. {'neg': 0.129, 'neu': 0.716, 'pos': 0.155, 'compound': 1.0}\n",
      "Day 289/374, positive_day=False, total sentiment: 115.37749999999929, combined text sentiment. {'neg': 0.15, 'neu': 0.694, 'pos': 0.156, 'compound': 0.9989}\n",
      "Day 290/374, positive_day=False, total sentiment: 172.30050000000028, combined text sentiment. {'neg': 0.128, 'neu': 0.735, 'pos': 0.138, 'compound': 1.0}\n",
      "Day 291/374, positive_day=False, total sentiment: -100.10360000000183, combined text sentiment. {'neg': 0.137, 'neu': 0.721, 'pos': 0.142, 'compound': -0.9978}\n",
      "Day 292/374, positive_day=True, total sentiment: -301.09969999999913, combined text sentiment. {'neg': 0.162, 'neu': 0.711, 'pos': 0.127, 'compound': -1.0}\n",
      "Day 293/374, positive_day=False, total sentiment: -244.22979999999703, combined text sentiment. {'neg': 0.152, 'neu': 0.697, 'pos': 0.151, 'compound': -1.0}\n",
      "Day 294/374, positive_day=True, total sentiment: 143.629999999998, combined text sentiment. {'neg': 0.138, 'neu': 0.713, 'pos': 0.148, 'compound': 1.0}\n",
      "Day 295/374, positive_day=False, total sentiment: 131.09570000000147, combined text sentiment. {'neg': 0.151, 'neu': 0.707, 'pos': 0.142, 'compound': -1.0}\n",
      "Day 296/374, positive_day=False, total sentiment: 0.08079999999864862, combined text sentiment. {'neg': 0.15, 'neu': 0.705, 'pos': 0.145, 'compound': -1.0}\n",
      "Day 297/374, positive_day=False, total sentiment: -1.6864000000003108, combined text sentiment. {'neg': 0.128, 'neu': 0.744, 'pos': 0.129, 'compound': -1.0}\n",
      "Day 298/374, positive_day=True, total sentiment: -161.3394000000009, combined text sentiment. {'neg': 0.152, 'neu': 0.709, 'pos': 0.14, 'compound': -1.0}\n",
      "Day 299/374, positive_day=False, total sentiment: 16.33609999999937, combined text sentiment. {'neg': 0.122, 'neu': 0.76, 'pos': 0.117, 'compound': -1.0}\n",
      "Day 300/374, positive_day=False, total sentiment: 144.27019999999794, combined text sentiment. {'neg': 0.133, 'neu': 0.729, 'pos': 0.138, 'compound': 1.0}\n",
      "Day 301/374, positive_day=True, total sentiment: -438.2284999999988, combined text sentiment. {'neg': 0.148, 'neu': 0.722, 'pos': 0.13, 'compound': -1.0}\n",
      "Day 302/374, positive_day=False, total sentiment: -216.45499999999947, combined text sentiment. {'neg': 0.145, 'neu': 0.715, 'pos': 0.14, 'compound': -1.0}\n",
      "Day 303/374, positive_day=True, total sentiment: -271.18739999999923, combined text sentiment. {'neg': 0.144, 'neu': 0.721, 'pos': 0.134, 'compound': -1.0}\n",
      "Day 304/374, positive_day=False, total sentiment: -41.48240000000154, combined text sentiment. {'neg': 0.139, 'neu': 0.719, 'pos': 0.142, 'compound': -0.9999}\n",
      "Day 305/374, positive_day=True, total sentiment: -93.14220000000121, combined text sentiment. {'neg': 0.147, 'neu': 0.721, 'pos': 0.131, 'compound': -1.0}\n",
      "Day 306/374, positive_day=False, total sentiment: -69.1580000000009, combined text sentiment. {'neg': 0.131, 'neu': 0.742, 'pos': 0.127, 'compound': -1.0}\n",
      "Day 307/374, positive_day=False, total sentiment: -228.74289999999968, combined text sentiment. {'neg': 0.139, 'neu': 0.734, 'pos': 0.127, 'compound': -1.0}\n",
      "Day 308/374, positive_day=True, total sentiment: -238.25989999999834, combined text sentiment. {'neg': 0.147, 'neu': 0.72, 'pos': 0.133, 'compound': -1.0}\n",
      "Day 309/374, positive_day=True, total sentiment: -371.01179999999874, combined text sentiment. {'neg': 0.151, 'neu': 0.708, 'pos': 0.141, 'compound': -1.0}\n",
      "Day 310/374, positive_day=True, total sentiment: -257.55170000000055, combined text sentiment. {'neg': 0.142, 'neu': 0.712, 'pos': 0.146, 'compound': 0.9999}\n",
      "Day 311/374, positive_day=False, total sentiment: -327.5006999999962, combined text sentiment. {'neg': 0.132, 'neu': 0.764, 'pos': 0.104, 'compound': -1.0}\n",
      "Day 312/374, positive_day=True, total sentiment: -494.2751999999979, combined text sentiment. {'neg': 0.152, 'neu': 0.713, 'pos': 0.135, 'compound': -1.0}\n",
      "Day 313/374, positive_day=False, total sentiment: -257.7590999999993, combined text sentiment. {'neg': 0.134, 'neu': 0.741, 'pos': 0.125, 'compound': -1.0}\n",
      "Day 314/374, positive_day=False, total sentiment: -239.2021000000003, combined text sentiment. {'neg': 0.138, 'neu': 0.728, 'pos': 0.134, 'compound': -1.0}\n",
      "Day 315/374, positive_day=True, total sentiment: -232.78310000000158, combined text sentiment. {'neg': 0.156, 'neu': 0.697, 'pos': 0.147, 'compound': -1.0}\n",
      "Day 316/374, positive_day=False, total sentiment: -290.9327999999982, combined text sentiment. {'neg': 0.155, 'neu': 0.702, 'pos': 0.143, 'compound': -1.0}\n",
      "Day 317/374, positive_day=True, total sentiment: -175.12480000000116, combined text sentiment. {'neg': 0.139, 'neu': 0.738, 'pos': 0.123, 'compound': -1.0}\n",
      "Day 318/374, positive_day=True, total sentiment: -272.30329999999856, combined text sentiment. {'neg': 0.156, 'neu': 0.693, 'pos': 0.151, 'compound': -1.0}\n",
      "Day 319/374, positive_day=True, total sentiment: -31.38980000000012, combined text sentiment. {'neg': 0.133, 'neu': 0.73, 'pos': 0.137, 'compound': 0.9998}\n",
      "Day 320/374, positive_day=True, total sentiment: -4.818399999999951, combined text sentiment. {'neg': 0.148, 'neu': 0.708, 'pos': 0.144, 'compound': -0.9999}\n",
      "Day 321/374, positive_day=False, total sentiment: 113.35059999999777, combined text sentiment. {'neg': 0.127, 'neu': 0.729, 'pos': 0.143, 'compound': 1.0}\n",
      "Day 322/374, positive_day=True, total sentiment: 155.34139999999945, combined text sentiment. {'neg': 0.13, 'neu': 0.739, 'pos': 0.131, 'compound': -0.9998}\n",
      "Day 323/374, positive_day=False, total sentiment: 254.92530000000204, combined text sentiment. {'neg': 0.143, 'neu': 0.698, 'pos': 0.16, 'compound': 1.0}\n",
      "Day 324/374, positive_day=True, total sentiment: 383.9812000000017, combined text sentiment. {'neg': 0.142, 'neu': 0.712, 'pos': 0.146, 'compound': 0.9995}\n",
      "Day 325/374, positive_day=True, total sentiment: 311.45960000000247, combined text sentiment. {'neg': 0.138, 'neu': 0.714, 'pos': 0.148, 'compound': 1.0}\n",
      "Day 326/374, positive_day=False, total sentiment: 20.942699999999995, combined text sentiment. {'neg': 0.103, 'neu': 0.702, 'pos': 0.196, 'compound': 0.9997}\n",
      "Day 327/374, positive_day=False, total sentiment: 3.5442, combined text sentiment. {'neg': 0.019, 'neu': 0.822, 'pos': 0.159, 'compound': 0.9791}\n",
      "Day 328/374, positive_day=True, total sentiment: 0.7539999999999999, combined text sentiment. {'neg': 0.057, 'neu': 0.842, 'pos': 0.101, 'compound': 0.8527}\n",
      "Day 329/374, positive_day=False, total sentiment: 1.4199, combined text sentiment. {'neg': 0.173, 'neu': 0.232, 'pos': 0.595, 'compound': 0.8363}\n",
      "Day 330/374, positive_day=True, total sentiment: 5.1168000000000005, combined text sentiment. {'neg': 0.029, 'neu': 0.814, 'pos': 0.157, 'compound': 0.9939}\n",
      "Day 331/374, positive_day=True, total sentiment: 0.7171999999999997, combined text sentiment. {'neg': 0.186, 'neu': 0.641, 'pos': 0.174, 'compound': -0.9128}\n",
      "Day 332/374, positive_day=False, total sentiment: -0.2304, combined text sentiment. {'neg': 0.235, 'neu': 0.555, 'pos': 0.21, 'compound': -0.4763}\n",
      "Day 333/374, positive_day=True, total sentiment: -0.0022999999999999687, combined text sentiment. {'neg': 0.131, 'neu': 0.759, 'pos': 0.11, 'compound': -0.1528}\n",
      "Day 334/374, positive_day=False, total sentiment: 1.0561, combined text sentiment. {'neg': 0.051, 'neu': 0.767, 'pos': 0.182, 'compound': 0.7806}\n",
      "Day 335/374, positive_day=False, total sentiment: 4.6041, combined text sentiment. {'neg': 0.115, 'neu': 0.724, 'pos': 0.161, 'compound': 0.9959}\n",
      "Day 336/374, positive_day=True, total sentiment: 2.9753000000000007, combined text sentiment. {'neg': 0.079, 'neu': 0.817, 'pos': 0.104, 'compound': 0.8895}\n",
      "Day 337/374, positive_day=True, total sentiment: 0.0, combined text sentiment. {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Day 338/374, positive_day=True, total sentiment: 1.6515, combined text sentiment. {'neg': 0.118, 'neu': 0.743, 'pos': 0.139, 'compound': 0.8331}\n",
      "Day 339/374, positive_day=True, total sentiment: 1.2503, combined text sentiment. {'neg': 0.098, 'neu': 0.709, 'pos': 0.193, 'compound': 0.947}\n",
      "Day 340/374, positive_day=False, total sentiment: 0.6943, combined text sentiment. {'neg': 0.0, 'neu': 0.945, 'pos': 0.055, 'compound': 0.6943}\n",
      "Day 341/374, positive_day=False, total sentiment: -0.5709, combined text sentiment. {'neg': 0.649, 'neu': 0.351, 'pos': 0.0, 'compound': -0.5709}\n",
      "Day 342/374, positive_day=True, total sentiment: 1.6320999999999999, combined text sentiment. {'neg': 0.089, 'neu': 0.519, 'pos': 0.391, 'compound': 0.9047}\n",
      "Day 343/374, positive_day=True, total sentiment: 0.7741, combined text sentiment. {'neg': 0.0, 'neu': 0.929, 'pos': 0.071, 'compound': 0.7741}\n",
      "Day 344/374, positive_day=True, total sentiment: -0.4939, combined text sentiment. {'neg': 0.144, 'neu': 0.856, 'pos': 0.0, 'compound': -0.4939}\n",
      "Day 345/374, positive_day=True, total sentiment: 1.1942, combined text sentiment. {'neg': 0.0, 'neu': 0.849, 'pos': 0.151, 'compound': 0.6124}\n",
      "Day 346/374, positive_day=False, total sentiment: 2.4107, combined text sentiment. {'neg': 0.101, 'neu': 0.705, 'pos': 0.195, 'compound': 0.9841}\n",
      "Day 347/374, positive_day=True, total sentiment: 0.0, combined text sentiment. {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Day 348/374, positive_day=True, total sentiment: -0.9571999999999999, combined text sentiment. {'neg': 0.276, 'neu': 0.554, 'pos': 0.17, 'compound': -0.8731}\n",
      "Day 349/374, positive_day=True, total sentiment: -0.4389, combined text sentiment. {'neg': 0.119, 'neu': 0.832, 'pos': 0.05, 'compound': -0.5386}\n",
      "Day 350/374, positive_day=False, total sentiment: 1.0144, combined text sentiment. {'neg': 0.113, 'neu': 0.576, 'pos': 0.311, 'compound': 0.8221}\n",
      "Day 351/374, positive_day=True, total sentiment: 1.295, combined text sentiment. {'neg': 0.111, 'neu': 0.587, 'pos': 0.303, 'compound': 0.8462}\n",
      "Day 352/374, positive_day=True, total sentiment: 0.7717, combined text sentiment. {'neg': 0.078, 'neu': 0.569, 'pos': 0.352, 'compound': 0.7717}\n",
      "Day 353/374, positive_day=True, total sentiment: 0.9497000000000001, combined text sentiment. {'neg': 0.033, 'neu': 0.748, 'pos': 0.218, 'compound': 0.8259}\n",
      "Day 354/374, positive_day=False, total sentiment: 0.4767, combined text sentiment. {'neg': 0.0, 'neu': 0.853, 'pos': 0.147, 'compound': 0.4767}\n",
      "Day 355/374, positive_day=True, total sentiment: -1.5254999999999999, combined text sentiment. {'neg': 0.241, 'neu': 0.669, 'pos': 0.091, 'compound': -0.9718}\n",
      "Day 356/374, positive_day=False, total sentiment: -0.5563, combined text sentiment. {'neg': 0.642, 'neu': 0.358, 'pos': 0.0, 'compound': -0.5563}\n",
      "Day 357/374, positive_day=False, total sentiment: 0.0, combined text sentiment. {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Day 358/374, positive_day=True, total sentiment: -0.5719, combined text sentiment. {'neg': 0.54, 'neu': 0.46, 'pos': 0.0, 'compound': -0.5719}\n",
      "Day 359/374, positive_day=False, total sentiment: -0.5423, combined text sentiment. {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.5423}\n",
      "Day 360/374, positive_day=True, total sentiment: 0.433, combined text sentiment. {'neg': 0.271, 'neu': 0.431, 'pos': 0.298, 'compound': 0.4404}\n",
      "Day 361/374, positive_day=True, total sentiment: -0.012499999999999956, combined text sentiment. {'neg': 0.156, 'neu': 0.68, 'pos': 0.164, 'compound': 0.3299}\n",
      "Day 362/374, positive_day=True, total sentiment: 0.765, combined text sentiment. {'neg': 0.0, 'neu': 0.476, 'pos': 0.524, 'compound': 0.765}\n",
      "Day 363/374, positive_day=True, total sentiment: 0.4215, combined text sentiment. {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
      "Day 364/374, positive_day=False, total sentiment: 1.7393999999999998, combined text sentiment. {'neg': 0.0, 'neu': 0.629, 'pos': 0.371, 'compound': 0.9117}\n",
      "Day 365/374, positive_day=True, total sentiment: 0.0, combined text sentiment. {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Day 366/374, positive_day=False, total sentiment: -0.22009999999999996, combined text sentiment. {'neg': 0.2, 'neu': 0.636, 'pos': 0.164, 'compound': -0.2695}\n",
      "Day 367/374, positive_day=True, total sentiment: 0.431, combined text sentiment. {'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.431}\n",
      "Day 368/374, positive_day=False, total sentiment: 2.2363, combined text sentiment. {'neg': 0.084, 'neu': 0.537, 'pos': 0.379, 'compound': 0.9485}\n",
      "Day 369/374, positive_day=True, total sentiment: 0.45330000000000004, combined text sentiment. {'neg': 0.111, 'neu': 0.663, 'pos': 0.226, 'compound': 0.4497}\n",
      "Day 370/374, positive_day=True, total sentiment: 1.1988, combined text sentiment. {'neg': 0.0, 'neu': 0.658, 'pos': 0.342, 'compound': 0.8316}\n",
      "Day 371/374, positive_day=True, total sentiment: 0.0, combined text sentiment. {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Day 372/374, positive_day=True, total sentiment: 0.0, combined text sentiment. {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "Day 373/374, positive_day=True, total sentiment: 0.3182, combined text sentiment. {'neg': 0.0, 'neu': 0.777, 'pos': 0.223, 'compound': 0.3182}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#mongo_result = db[\"analysis_with_custom_lexicon\"]\n",
    "\n",
    "mongo_result = db[\"analysis_with_custom_lexicon\"]\n",
    "mongo_result.create_index([(\"date\", ASCENDING)])\n",
    "\n",
    "dates_to_ignore = mongo_result.distinct(\"date\")\n",
    "\n",
    "for date in dates:\n",
    "    if date in dates_to_ignore:\n",
    "        continue\n",
    "            \n",
    "    comments = mongo_comments.find({\"next_trading_day\": date})\n",
    "    positive_sentiment = 0\n",
    "    negative_sentiment = 0\n",
    "    neutral_sentiment = 0\n",
    "    total_sentiment = 0\n",
    "    text = \"\"\n",
    "    for comment in comments:\n",
    "        positive_day = comment[\"next_trading_day_positive\"]\n",
    "\n",
    "        text += comment[\"body\"] + \" \"\n",
    "\n",
    "        sentiment = sid.polarity_scores(comment[\"body\"])[\"compound\"]\n",
    "        total_sentiment += sentiment\n",
    "        if sentiment > 0:\n",
    "            positive_sentiment += 1\n",
    "        elif sentiment < 0:\n",
    "            negative_sentiment += 1\n",
    "        else:\n",
    "            neutral_sentiment += 1\n",
    "    combined_text_sentiment = sid.polarity_scores(text)\n",
    "    result = {\n",
    "        \"date\": date,\n",
    "        \"positive_day\": positive_day,\n",
    "        \"positive_sentiment\": positive_sentiment,\n",
    "        \"negative_sentiment\": negative_sentiment,\n",
    "        \"neutral_sentiment\": neutral_sentiment,\n",
    "        \"total_sentiment\": total_sentiment,\n",
    "        \"combined_text_sentiment\": combined_text_sentiment\n",
    "    }\n",
    "    mongo_result.delete_many({\"date\": date})\n",
    "    mongo_result.insert_one(result)\n",
    "    print(f\"Day {dates.index(date)}/{len(dates)}, {positive_day=}, total sentiment: {total_sentiment}, combined text sentiment. {combined_text_sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ammount of positive days: 215\n",
      "Ammount of negative days: 153\n",
      "\n",
      "average ratio of positive comments on a positive day: 0.5729518272466309\n",
      "average ratio of positive comments on a negative day: 0.570918550404411\n",
      "\n",
      "average sentiment on a positive day: 0.052820455865656\n",
      "average sentiment on a negative day: 0.0477101676048771\n",
      "\n",
      "average combined sentiment on positive days 0.6610883720930233\n",
      "average combined sentiment on negative days 0.6361019607843137\n"
     ]
    }
   ],
   "source": [
    "average_sentiment_positive = 0\n",
    "average_sentiment_negative = 0\n",
    "average_sentiment_neutral = 0\n",
    "negative_days = 0\n",
    "positive_days = 0\n",
    "\n",
    "average_combined_sentiment_positive_days = 0\n",
    "average_combined_sentiment_negative_days = 0\n",
    "\n",
    "total_sentiment_positive_days = 0\n",
    "total_sentiment_negative_days = 0\n",
    "\n",
    "mongo_result = db[\"analysis_with_custom_lexicon\"]\n",
    "results = mongo_result.find()\n",
    "\n",
    "for result in results:\n",
    "    positive_day = result[\"positive_day\"]\n",
    "    positive_sentiment = result[\"positive_sentiment\"]\n",
    "    negative_sentiment = result[\"negative_sentiment\"]\n",
    "    neutral_sentiment = result[\"neutral_sentiment\"]\n",
    "    total_sentiment = result[\"total_sentiment\"]\n",
    "    combined_text_sentiment = result[\"combined_text_sentiment\"]\n",
    "    if positive_sentiment + negative_sentiment > 0:\n",
    "        if positive_day:\n",
    "            positive_days += 1\n",
    "            average_sentiment_positive += positive_sentiment / (positive_sentiment + negative_sentiment)\n",
    "            average_combined_sentiment_positive_days += combined_text_sentiment[\"compound\"]\n",
    "            total_sentiment_positive_days += total_sentiment / (positive_sentiment + negative_sentiment + neutral_sentiment)\n",
    "        else:\n",
    "            negative_days += 1\n",
    "            average_sentiment_negative += positive_sentiment / (positive_sentiment + negative_sentiment)\n",
    "            average_combined_sentiment_negative_days += combined_text_sentiment[\"compound\"]\n",
    "            total_sentiment_negative_days += total_sentiment / (positive_sentiment + negative_sentiment + neutral_sentiment)\n",
    "            \n",
    "average_sentiment_positive = average_sentiment_positive / positive_days\n",
    "average_sentiment_negative = average_sentiment_negative / negative_days\n",
    "\n",
    "average_combined_sentiment_positive_days = average_combined_sentiment_positive_days / positive_days \n",
    "average_combined_sentiment_negative_days = average_combined_sentiment_negative_days / negative_days\n",
    "\n",
    "total_sentiment_positive_days = total_sentiment_positive_days / positive_days\n",
    "total_sentiment_negative_days = total_sentiment_negative_days / negative_days\n",
    "\n",
    "print(f\"Ammount of positive days: {positive_days}\")\n",
    "print(f\"Ammount of negative days: {negative_days}\")\n",
    "print()\n",
    "print(f\"average ratio of positive comments on a positive day: {average_sentiment_positive}\")\n",
    "print(f\"average ratio of positive comments on a negative day: {average_sentiment_negative}\")\n",
    "print()\n",
    "print(f\"average sentiment on a positive day: {total_sentiment_positive_days}\")\n",
    "print(f\"average sentiment on a negative day: {total_sentiment_negative_days}\")\n",
    "print()\n",
    "print(f\"average combined sentiment on positive days {average_combined_sentiment_positive_days}\")\n",
    "print(f\"average combined sentiment on negative days {average_combined_sentiment_negative_days}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive_sentiment_positive_days=0.34726655782922766\n",
      "neutral_sentiment_positive_days=0.3996415962530291\n",
      "negative_sentiment_positive_days=0.25309184591774325\n",
      "positive_sentiment_negative_days=0.3541152130992973\n",
      "neutral_sentiment_negative_days=0.3800990609830611\n",
      "negative_sentiment_negative_days=0.2657857259176414\n"
     ]
    }
   ],
   "source": [
    "mongo_result = db[\"analysis_without_custom_lexicon\"]\n",
    "results = mongo_result.find()\n",
    "positive_sentiment_positive_days = 0\n",
    "neutral_sentiment_positive_days = 0\n",
    "negative_sentiment_positive_days = 0\n",
    "positive_sentiment_negative_days = 0\n",
    "neutral_sentiment_negative_days = 0\n",
    "negative_sentiment_negative_days = 0\n",
    "positive_days = 0\n",
    "negative_days = 0\n",
    "for result in results:\n",
    "    positive_day = result[\"positive_day\"]\n",
    "    positive_sentiment = result[\"positive_sentiment\"]\n",
    "    negative_sentiment = result[\"negative_sentiment\"]\n",
    "    neutral_sentiment = result[\"neutral_sentiment\"]\n",
    "    total_sentiment = result[\"total_sentiment\"]\n",
    "    combined_text_sentiment = result[\"combined_text_sentiment\"]\n",
    "    if positive_day:\n",
    "        positive_days += 1\n",
    "        positive_sentiment_positive_days += positive_sentiment / (neutral_sentiment + positive_sentiment + negative_sentiment)\n",
    "        neutral_sentiment_positive_days += neutral_sentiment / (neutral_sentiment + positive_sentiment + negative_sentiment)\n",
    "        negative_sentiment_positive_days += negative_sentiment / (neutral_sentiment + positive_sentiment + negative_sentiment)\n",
    "    else:\n",
    "        negative_days += 1\n",
    "        positive_sentiment_negative_days += positive_sentiment / (neutral_sentiment + positive_sentiment + negative_sentiment)\n",
    "        neutral_sentiment_negative_days += neutral_sentiment / (neutral_sentiment + positive_sentiment + negative_sentiment)\n",
    "        negative_sentiment_negative_days += negative_sentiment / (neutral_sentiment + positive_sentiment + negative_sentiment)\n",
    "        \n",
    "positive_sentiment_positive_days = positive_sentiment_positive_days / positive_days\n",
    "neutral_sentiment_positive_days = neutral_sentiment_positive_days / positive_days\n",
    "negative_sentiment_positive_days = negative_sentiment_positive_days / positive_days\n",
    "positive_sentiment_negative_days = positive_sentiment_negative_days / negative_days\n",
    "neutral_sentiment_negative_days = neutral_sentiment_negative_days / negative_days\n",
    "negative_sentiment_negative_days = negative_sentiment_negative_days / negative_days\n",
    "        \n",
    "print(f\"{positive_sentiment_positive_days=}\")\n",
    "print(f\"{neutral_sentiment_positive_days=}\")\n",
    "print(f\"{negative_sentiment_positive_days=}\")\n",
    "print(f\"{positive_sentiment_negative_days=}\")\n",
    "print(f\"{neutral_sentiment_negative_days=}\")\n",
    "print(f\"{negative_sentiment_negative_days=}\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "count = 0\n",
    "for comment in mongo_comments.find():\n",
    "    count += 1\n",
    "    words = []\n",
    "    for word in comment['body'].split():\n",
    "        if not (word.startswith('/u/') or word.startswith('/r/')):\n",
    "            words.append(word)\n",
    "    text = \" \".join(words).encode(\"ascii\", \"ignore\").decode()\n",
    "    doc = nlp(\" \".join(words))\n",
    "    processed = []\n",
    "    for token in doc:\n",
    "        #print(token)\n",
    "        if token.lemma_.isalpha():\n",
    "            processed.append(token.lemma_.lower())\n",
    "    \n",
    "    mongo_comments.update_one({\"id\": comment[\"id\"],\"post_id\": comment[\"post_id\"]},{\"$set\": {\"preprocessed_comment\": processed}})\n",
    "    processed=[]\n",
    "    for token in doc:\n",
    "        if token.lemma_.isalpha() and not token.is_stop:\n",
    "            processed.append(token.lemma_.lower())\n",
    "    \n",
    "    mongo_comments.update_one({\"id\": comment[\"id\"],\"post_id\": comment[\"post_id\"]},{\"$set\": {\"preprocessed_comment_without_stop\": processed}})\n",
    "    \n",
    "    if count % 100 == 0:\n",
    "        print(f\"comment number {count}: {processed}\")\n",
    "\n",
    "    #processed = [token.lemma_ for token in doc if token.lemma_.isalpha() and not (token.is_space or token.like_num or token.like_url or str(token).startswith('/u/')) ]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data and split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def no_filter_iterator(field):\n",
    "    cursor = mongo_comments.find() \n",
    "    for comment in cursor:\n",
    "        if \"comment\" in field:\n",
    "            yield \" \".join(comment[field]).encode(\"ascii\", \"ignore\").decode()\n",
    "        else:\n",
    "            yield comment[field]\n",
    "        \n",
    "def min_10_upvotes_iterator(field, threshold = 10):\n",
    "    cursor = mongo_comments.find() \n",
    "    for comment in cursor:\n",
    "        if comment[\"score\"] > threshold:\n",
    "            if \"comment\" in field:\n",
    "                yield \" \".join(comment[field]).encode(\"ascii\", \"ignore\").decode()\n",
    "            else:\n",
    "                yield comment[field]\n",
    "\n",
    "def min_5_upvotes_iterator(field, threshold = 1):\n",
    "    cursor = mongo_comments.find() \n",
    "    for comment in cursor:\n",
    "        if comment[\"score\"] > threshold:\n",
    "            if \"comment\" in field:\n",
    "                yield \" \".join(comment[field]).encode(\"ascii\", \"ignore\").decode()\n",
    "            else:\n",
    "                yield comment[field]\n",
    "\n",
    "\n",
    "                \n",
    "iterator = no_filter_iterator                               \n",
    "#iterator = min_10_upvotes_iterator\n",
    "#iterator = min_5_upvotes_iterator\n",
    "    \n",
    "X_data = np.fromiter(iter(iterator(\"preprocessed_comment_without_stop\")), 'S1000')\n",
    "Y_data = np.fromiter(iter(iterator(\"next_trading_day_positive\")), bool)\n",
    "\n",
    "\n",
    "number_of_comments = len(X_data)\n",
    "\n",
    "train_threshold = 0.75\n",
    "train_end = int(train_threshold * number_of_comments)\n",
    "\n",
    "\n",
    "indices = np.arange(0, number_of_comments)\n",
    "np.random.shuffle(indices)\n",
    "    \n",
    "X_data = X_data[indices]\n",
    "Y_data = Y_data[indices]\n",
    "\n",
    "\n",
    "X_train = X_data[:train_end]\n",
    "Y_train = Y_data[:train_end]\n",
    "\n",
    "X_test = X_data[train_end:]\n",
    "Y_test = Y_data[train_end:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'will bounce on dick all night bb',\n",
       "       b'everyone back to bear gang already that be fast',\n",
       "       b'oh shit how many PEOPLE stick at home have SEX NEW GENERATION call the QUARENTEENS move aside millennial boomer and bitch ass generation z',\n",
       "       ..., b'why do look like forget who the first lady be Lmfao',\n",
       "       b'bunch of boomer in the Senate invade each other space Corona yes yes close close',\n",
       "       b'see all these bull everywhere actually make feel optimistic about the put i get today let go'],\n",
       "      dtype='|S1000')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for x in X_train:\n",
    "    lengths.append(len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(sorted(lengths))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "length_checker = np.vectorize(len)\n",
    "train_len = length_checker(X_train)\n",
    "test_len = length_checker(X_test)\n",
    "X_train = X_train[train_len > 150]\n",
    "Y_train = Y_train[train_len > 150]\n",
    "X_test = X_test[test_len > 150]\n",
    "Y_test = Y_test[test_len > 150]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data score: 0.5881272660554243\n",
      "Test data score: 0.5473998468602483\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "steps= [('vec', CountVectorizer()), ('multinomialnb', MultinomialNB())]\n",
    "bayes_pipeline = Pipeline(steps)\n",
    "bayes_pipeline.fit(X_train, Y_train)\n",
    "print(f\"Train data score: {bayes_pipeline.score(X_train, Y_train)}\")\n",
    "print(f\"Test data score: {bayes_pipeline.score(X_test, Y_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "steps = [('vec', CountVectorizer()), ('randomforest', RandomForestClassifier(n_estimators=200, criterion='entropy', verbose=10, n_jobs=-1))]\n",
    "\n",
    "forest_pipeline = Pipeline(steps, verbose=True)\n",
    "forest_pipeline.fit(X_train, Y_train)\n",
    "print(f\"Train data score: {forest_pipeline.score(X_train, Y_train)}\")\n",
    "print(f\"Test data score: {forest_pipeline.score(X_test, Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-006c51581650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moov_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moov_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "if len(tf.config.list_physical_devices('GPU')):\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "    \n",
    "\n",
    "vocab_size = 30000\n",
    "embedding_dim = 100\n",
    "oov_tok = '<OOV>'\n",
    "max_length = 350\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train.astype(str))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train.astype(str))\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    \n",
    "validation_sequences = tokenizer.texts_to_sequences(X_test.astype(str))\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "\n",
    "training_label_seq = np.array(list(map(int,Y_train)))\n",
    "validation_label_seq = np.array(list(map(int, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385/385 [==============================] - 52s 134ms/step - loss: 8.3308 - accuracy: 0.4599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f09ba180340>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
    "\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim))\n",
    "    model.add(LSTM(units=64, return_sequences=True, dropout=0.2))\n",
    "    model.add(LSTM(units=1, dropout=0.2))\n",
    "    model.add(Dense(units=1, activation='relu'))\n",
    "    # Adding the output layer\n",
    "    model.add(Dense(units=1))\n",
    "\n",
    "    # Compiling the RNN\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "model.fit(train_padded, Y_train, epochs=1, batch_size=3024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5762153 -0.5762153 -0.5762153 ... -0.5762153 -0.5762153 -0.5762153]\n"
     ]
    }
   ],
   "source": [
    "probabilities = np.squeeze(model.predict(validation_padded))\n",
    "predictions = np.where(probabilities > 0.5, 1, 0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12122/12122 [==============================] - 99s 8ms/step - loss: 8.3052 - accuracy: 0.4616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.305191993713379, 0.4615693986415863]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(validation_padded, validation_label_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.46156939886202214\n",
      "rate of positive predictions: 0.0\n",
      "rate of positive validation labels (baseline): 0.5384306011379779\n",
      "rate of negative validation labels (baseline): 0.4615693988620221\n"
     ]
    }
   ],
   "source": [
    "\n",
    "right_classified = 0\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == validation_label_seq[i]:\n",
    "        right_classified += 1\n",
    "        \n",
    "print(f\"accuracy: {right_classified/len(predictions)}\")\n",
    "print(f\"rate of positive predictions: {sum(predictions)/len(predictions)}\" )\n",
    "print(f\"rate of positive validation labels (baseline): {sum(validation_label_seq)/len(validation_label_seq)}\")\n",
    "print(f\"rate of negative validation labels (baseline): {1 - sum(validation_label_seq)/len(validation_label_seq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining upvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "threshold = 10\n",
    "average_length = 0\n",
    "for comment in mongo_comments.find():\n",
    "    if comment[\"score\"] >= threshold:\n",
    "        scores.append(comment[\"score\"])\n",
    "        average_length += len(comment[\"body\"].split())\n",
    "average_length = average_length / len(scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23080\n",
      "22.10966204506066\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(scores))\n",
    "print(average_length)\n",
    "plt.plot(sorted(scores))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proper cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = [10000, 100000]\n",
    "filter_stopwords = [True, False]\n",
    "embedding_dims = [100, 200]\n",
    "lstm_layers_ammounts  = [1, 2]\n",
    "nodes_in_lstm_layers = [50,100]\n",
    "max_lengths = [100, 300, None]\n",
    "min_lengths = [0, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "\n",
    "for size in vocab_size:\n",
    "    for should_filter in filter_stopwords:\n",
    "        for embedding_dim in embedding_dims:\n",
    "            for n_lstm_layers in lstm_layers_ammounts:\n",
    "                for n_lstm_nodes in nodes_in_lstm_layers:\n",
    "                    for max_len in max_lengths:\n",
    "                        if max_len is None:\n",
    "                            for min_len in min_lengths:\n",
    "                                runs.append({\n",
    "                                    \"vocab_size\": size,\n",
    "                                    \"filter_stopwords\": should_filter,\n",
    "                                    \"embedding_dim\": embedding_dim,\n",
    "                                    \"n_lstm_layers\": n_lstm_layers,\n",
    "                                    \"n_lstm_nodes\" : n_lstm_nodes,\n",
    "                                    \"max_len\": None,\n",
    "                                    \"min_len\": min_len\n",
    "                                })\n",
    "                        else:\n",
    "                            runs.append({\n",
    "                                \"vocab_size\": size,\n",
    "                                \"filter_stopwords\": should_filter,\n",
    "                                \"embedding_dim\": embedding_dim,\n",
    "                                \"n_lstm_layers\": n_lstm_layers,\n",
    "                                \"n_lstm_nodes\" : n_lstm_nodes,\n",
    "                                \"max_len\": max_len,\n",
    "                                \"min_len\": 0\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def min_5_upvotes_iterator(field, threshold = 5):\n",
    "    cursor = mongo_comments.find() \n",
    "    for comment in cursor:\n",
    "        if comment[\"score\"] > threshold:\n",
    "            if \"comment\" in field:\n",
    "                yield \" \".join(comment[field]).encode(\"ascii\", \"ignore\").decode()\n",
    "            else:\n",
    "                yield comment[field]\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "X_data_without_stop = np.fromiter(iter(min_5_upvotes_iterator(\"preprocessed_comment_without_stop\")), 'S1000')\n",
    "X_data = np.fromiter(iter(min_5_upvotes_iterator(\"preprocessed_comment\")), 'S1000')\n",
    "Y_data = np.fromiter(iter(min_5_upvotes_iterator(\"next_trading_day_positive\")), bool)\n",
    "\n",
    "number_of_comments = len(X_data)\n",
    "\n",
    "if sum(Y_data)/number_of_comments > 0.5:\n",
    "    \n",
    "    number_to_remove = sum(Y_data) - (number_of_comments-sum(Y_data))\n",
    "    indexes_to_remove = np.random.choice(np.where(Y_data == 1)[0], number_to_remove, replace = False)\n",
    "else:\n",
    "    number_to_remove = (number_of_comments-sum(Y_data)) - sum(Y_data)\n",
    "    indexes_to_remove = np.random.choice(np.where(Y_data == 0)[0], number_to_remove, replace = False)\n",
    "\n",
    "X_data_without_stop = np.delete(X_data_without_stop, indexes_to_remove)\n",
    "X_data = np.delete(X_data, indexes_to_remove)\n",
    "Y_data = np.delete(Y_data, indexes_to_remove)\n",
    "\n",
    "number_of_comments = len(X_data)\n",
    "indices = np.arange(0, number_of_comments)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "X_data_without_stop = X_data_without_stop[indices]\n",
    "X_data = X_data[indices]\n",
    "Y_data = Y_data[indices]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68926\n"
     ]
    }
   ],
   "source": [
    "def get_data(filter_stopwords, min_len, max_len):\n",
    "    number_of_comments = len(X_data)\n",
    "\n",
    "    train_threshold = 0.70\n",
    "    train_end = int(train_threshold * number_of_comments)\n",
    "    \n",
    "    valid_threshold = 0.85\n",
    "    valid_end = int(valid_threshold * number_of_comments)\n",
    "\n",
    "    if filter_stopwords:\n",
    "        x_train = X_data_without_stop[:train_end]\n",
    "        x_test = X_data_without_stop[train_end:valid_end]\n",
    "    else:\n",
    "        x_train = X_data[:train_end]\n",
    "        x_test = X_data[train_end:valid_end]\n",
    "\n",
    "    y_train = Y_data[:train_end]\n",
    "    y_test = Y_data[train_end:valid_end]\n",
    "    \n",
    "\n",
    "    length_checker = np.vectorize(len)\n",
    "    train_len = length_checker(x_train)\n",
    "    test_len = length_checker(x_test)\n",
    "\n",
    "    if max_len is not None:\n",
    "        x_train = x_train[train_len < max_len]\n",
    "        y_train = y_train[train_len < max_len]\n",
    "        x_test = x_test[test_len < max_len]\n",
    "        y_test = y_test[test_len < max_len]\n",
    "\n",
    "    elif min_len > 0:\n",
    "        x_train = x_train[train_len > min_len]\n",
    "        y_train = y_train[train_len > min_len]\n",
    "        x_test = x_test[test_len > min_len]\n",
    "        y_test = y_test[test_len > min_len]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "print(len(X_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_cv_results = db[\"cv_results\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for run in runs:\n",
    "    vocab_size = run[\"vocab_size\"]\n",
    "    filter_stopwords = run[\"filter_stopwords\"]\n",
    "    embedding_dim = run[\"embedding_dim\"]\n",
    "    n_lstm_layers = run[\"n_lstm_layers\"]\n",
    "    n_lstm_nodes = run[\"n_lstm_nodes\"]\n",
    "    max_len = run[\"max_len\"]\n",
    "    min_len = run[\"min_len\"]\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = get_data(filter_stopwords, min_len, max_len)\n",
    "    \n",
    "    \n",
    "    if len(tf.config.list_physical_devices('GPU')):\n",
    "        physical_devices = tf.config.list_physical_devices('GPU')\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "    \n",
    "    oov_tok = '<OOV>'\n",
    "    max_length = 500\n",
    "    trunc_type = 'post'\n",
    "    padding_type = 'post'\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(x_train.astype(str))\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    train_sequences = tokenizer.texts_to_sequences(x_train.astype(str))\n",
    "    train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    validation_sequences = tokenizer.texts_to_sequences(x_test.astype(str))\n",
    "    validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "\n",
    "    training_label_seq = np.array(list(map(int,y_train)))\n",
    "    validation_label_seq = np.array(list(map(int, y_test)))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim))\n",
    "    \n",
    "    for i in range(n_lstm_layers):\n",
    "        if n_lstm_layers > 1 and i+1 != n_lstm_layers:\n",
    "            model.add(LSTM(units=n_lstm_nodes, return_sequences=True, dropout=0.1))\n",
    "        else:\n",
    "            model.add(LSTM(units=n_lstm_nodes, dropout=0.1))\n",
    "    \n",
    "    model.add(Dense(units=32, activation='relu'))\n",
    "    model.add(Dense(units=1))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_padded, y_train, epochs=3, batch_size=32)\n",
    "    loss, accuracy = model.evaluate(validation_padded, validation_label_seq)\n",
    "    probabilities = np.squeeze(model.predict(validation_padded))\n",
    "    predictions = np.where(probabilities > 0.5, 1, 0)\n",
    "    results = {\n",
    "        \"parameters\": run,\n",
    "        \"loss\": loss,\n",
    "        \"accuracy\": accuracy,\n",
    "    }\n",
    "    print(f\"------------Results-------------\")\n",
    "    print(\"parameters:\", run)\n",
    "    print(\"loss:\", loss)\n",
    "    print(\"accuracy\", accuracy)\n",
    "    mongo_cv_results.insert_one(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data score: 0.7011482341236942\n",
      "Test data score: 0.5578876100203114\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.57      0.56      5124\n",
      "           1       0.56      0.55      0.55      5215\n",
      "\n",
      "    accuracy                           0.56     10339\n",
      "   macro avg       0.56      0.56      0.56     10339\n",
      "weighted avg       0.56      0.56      0.56     10339\n",
      "\n",
      "[[2923 2201]\n",
      " [2370 2845]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "x_train, y_train, x_test, y_test = get_data(filter_stopwords = True, min_len = 0, max_len = None)\n",
    "\n",
    "steps= [('vec', TfidfVectorizer()), ('multinomialnb', MultinomialNB())]\n",
    "bayes_pipeline = Pipeline(steps)\n",
    "bayes_pipeline.fit(x_train, y_train)\n",
    "print(f\"Train data score: {bayes_pipeline.score(x_train, y_train)}\")\n",
    "print(f\"Test data score: {bayes_pipeline.score(x_test, y_test)}\")\n",
    "\n",
    "pred = bayes_pipeline.predict(x_test)\n",
    "print(classification_report(y_test, pred, labels =[0, 1]))\n",
    "print(confusion_matrix(y_test, pred, labels=[0,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............... (step 1 of 2) Processing vec, total=   0.3s\n",
      "[Pipeline] ...... (step 2 of 2) Processing randomforest, total=  17.3s\n",
      "Train data score: 0.9820303432266623\n",
      "Test data score: 0.5442499274591354\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "steps = [('vec', CountVectorizer()), ('randomforest', RandomForestClassifier(n_estimators=200, criterion='entropy', verbose=0, n_jobs=-1))]\n",
    "x_train, y_train, x_test, y_test = get_data(filter_stopwords = False, min_len = 0, max_len = None)\n",
    "\n",
    "forest_pipeline = Pipeline(steps, verbose=True)\n",
    "forest_pipeline.fit(x_train, y_train)\n",
    "\n",
    "print(f\"Train data score: {forest_pipeline.score(x_train, y_train)}\")\n",
    "print(f\"Test data score: {forest_pipeline.score(x_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5107857491459249\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   9/1494 [..............................] - ETA: 4:25 - loss: 7.6054 - accuracy: 0.5069"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-b5d263428091>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalidation_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_label_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_label_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 100000\n",
    "filter_stopwords = True\n",
    "embedding_dim = 10\n",
    "max_len = 300\n",
    "min_len = 0\n",
    "\n",
    "x_train, y_train, x_test, y_test = get_data(filter_stopwords, min_len, max_len)\n",
    "\n",
    "\n",
    "if len(tf.config.list_physical_devices('GPU')):\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "\n",
    "oov_tok = '<OOV>'\n",
    "max_length = 300\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(x_train.astype(str))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train.astype(str))\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "validation_sequences = tokenizer.texts_to_sequences(x_test.astype(str))\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "\n",
    "training_label_seq = np.array(list(map(int,y_train)))\n",
    "validation_label_seq = np.array(list(map(int, y_test)))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim))\n",
    "\n",
    "model.add(LSTM(units=50, return_sequences=True, dropout=0))\n",
    "model.add(LSTM(units=50, return_sequences=True, dropout=0))\n",
    "model.add(LSTM(units=50, return_sequences=True, dropout=0))\n",
    "model.add(LSTM(units=50, dropout=0))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_padded, y_train,validation_data = (validation_padded, validation_label_seq), epochs=10, batch_size=32)\n",
    "loss, accuracy = model.evaluate(validation_padded, validation_label_seq)\n",
    "probabilities = np.squeeze(model.predict(validation_padded))\n",
    "\n",
    "\n",
    "predictions = np.where(probabilities > 0.5, 1, 0)\n",
    "results = {\n",
    "    \"parameters\": run,\n",
    "    \"loss\": loss,\n",
    "    \"accuracy\": accuracy,\n",
    "}\n",
    "print(f\"------------Results-------------\")\n",
    "print(\"parameters:\", run)\n",
    "print(\"loss:\", loss)\n",
    "print(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1551522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-98ed78db9d0c>:1: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  print(mongo_comments.find().count())\n"
     ]
    }
   ],
   "source": [
    "print(mongo_comments.find().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1893,  562,  172, ...,    0,    0,    0],\n",
       "       [  11,  829,   17, ...,    0,    0,    0],\n",
       "       [9982,  141,    0, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 318,  219,  658, ...,    0,    0,    0],\n",
       "       [1291,  815,    0, ...,    0,    0,    0],\n",
       "       [ 596,  897, 4534, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('5ffb425c78dd39d999d19678'), 'parameters': {'vocab_size': 10000, 'filter_stopwords': True, 'embedding_dim': 100, 'n_lstm_layers': 1, 'n_lstm_nodes': 50, 'max_len': 100, 'min_len': 0}, 'loss': 0.6933472752571106, 'accuracy': 0.49875903129577637}\n",
      "{'_id': ObjectId('5ffb42b078dd39d999d19679'), 'parameters': {'vocab_size': 10000, 'filter_stopwords': True, 'embedding_dim': 100, 'n_lstm_layers': 1, 'n_lstm_nodes': 50, 'max_len': 300, 'min_len': 0}, 'loss': 7.677075386047363, 'accuracy': 0.5022947192192078}\n",
      "{'_id': ObjectId('5ffb430478dd39d999d1967a'), 'parameters': {'vocab_size': 10000, 'filter_stopwords': True, 'embedding_dim': 100, 'n_lstm_layers': 1, 'n_lstm_nodes': 50, 'max_len': None, 'min_len': 0}, 'loss': 7.672938346862793, 'accuracy': 0.5025631189346313}\n",
      "{'_id': ObjectId('5ffb432178dd39d999d1967b'), 'parameters': {'vocab_size': 10000, 'filter_stopwords': True, 'embedding_dim': 100, 'n_lstm_layers': 1, 'n_lstm_nodes': 50, 'max_len': None, 'min_len': 50}, 'loss': 0.6931391954421997, 'accuracy': 0.5094283819198608}\n",
      "{'_id': ObjectId('5ffb432b78dd39d999d1967c'), 'parameters': {'vocab_size': 10000, 'filter_stopwords': True, 'embedding_dim': 100, 'n_lstm_layers': 1, 'n_lstm_nodes': 50, 'max_len': None, 'min_len': 100}, 'loss': 0.6925566792488098, 'accuracy': 0.5175355672836304}\n"
     ]
    }
   ],
   "source": [
    "mongo_cv_results = db[\"cv_results\"]\n",
    "best_accuracy = 0\n",
    "\n",
    "for result in mongo_cv_results.find():\n",
    "    if result[\"accuracy\"] > best_accuracy:\n",
    "        best_accuracy = result[\"accuracy\"]\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'amrn crush earning barely budge post market fml guess hold call till expiry'\n",
      " b'future bulls fucking retard fun homeless' b'edgelord tonight' ...\n",
      " b'send mod course video lick homeless mans r wallstreetbet comment reverse vacation'\n",
      " b'll ally' b'weekly reminder lebron james selfish piece shit']\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
